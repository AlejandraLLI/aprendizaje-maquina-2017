[
["index.html", "Aprendizaje de máquina Temario y referencias", " Aprendizaje de máquina Felipe González 2017-08-21 Temario y referencias Todas las notas y material del curso estarán en este repositorio. Introducción al aprendizaje máquina Regresión lineal múltiple y descenso en gradiente Problemas de clasificación y regresión logística Validación cruzada y métodos de remuestreo Regularización y selección de modelos Redes neuronales Árboles y bosques aleatorios Máquinas de soporte vectorial Diagnóstico y mejora en problemas de aprendizaje supervisado. Componentes principales Análisis de conglomerados Evaluación Tareas semanales (20%) Examen parcial (30% práctico, 20% teórico) Un examen final (30% práctico) Software: R y Rstudio R Sitio de R (CRAN) Rstudio Interfaz gráfica para trabajar en R. Recursos para aprender R Referencias principales An Introduction to Statistical Learning, James et al. (2014) Curso de Machine Learning de Andrew Ng, Ng (2017) Otras referencias Pattern Recognition and Machine Learning The Elements of Statistical Learning Otros materiales Statistical Modeling: The Two Cultures (Breiman 2001) References "],
["introduccion.html", "Clase 1 Introducción 1.1 ¿Qué es aprendizaje de máquina (machine learning)? 1.2 Aprendizaje Supervisado 1.3 Predicciones 1.4 Cuantificación de error o precisión 1.5 Tarea de aprendizaje supervisado 1.6 ¿Por qué tenemos errores? 1.7 ¿Cómo estimar f? 1.8 Resumen 1.9 Tarea", " Clase 1 Introducción 1.1 ¿Qué es aprendizaje de máquina (machine learning)? Métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. En este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Ejemplos de tareas de aprendizaje: Predecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses. Reconocer palabras escritas a mano (OCR). Detectar llamados de ballenas en grabaciones de boyas. Estimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica. Dividir a los clientes de Netflix según sus gustos. Recomendar artículos a clientes de un programa de lealtad o servicio online. Las razones usuales para intentar resolver estos problemas computacionalmente son diversas: Quisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo. Quisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo. Queremos entender de manera más completa y sistemática el comportamiento de un fenómeno, identificando variables o patrones importantes. Es posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc) Las razones para intentar usar aprendizaje para producir reglas en lugar de intentar construir estas reglas directamente son, por ejemplo: Cuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.) Reglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico.) Ejemplo: reconocimiento de dígitos escritos a mano ¿Cómo reconocer los siguientes dígitos de manera automática? En los datos tenemos los valores de cada pixel (los caracteres son imagenes de 16x16 pixeles), y una etiqueta asociada, que es el número que la imagen representa. Podemos ver las imágenes y las etiquetas: library(dplyr) library(tidyr) library(purrr) library(readr) zip_train &lt;- read_csv(file = &#39;datos/zip-train.csv&#39;) muestra_1 &lt;- sample_n(zip_train, 10) graficar_digitos(muestra_1) muestra_2 &lt;- sample_n(zip_train, 10) graficar_digitos(muestra_2) Los 16x16=256 están escritos acomodando las filas de la imagen en vector de 256 valores (cada renglón de zip_train). Un dígito entonces se representa como sigue: dim(zip_train) ## [1] 7291 257 as.numeric(zip_train[1,]) ## [1] 6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631 0.862 ## [11] -0.167 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [21] -1.000 -1.000 -1.000 -0.992 0.297 1.000 0.307 -1.000 -1.000 -1.000 ## [31] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.410 ## [41] 1.000 0.986 -0.565 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [51] -1.000 -1.000 -1.000 -1.000 -0.683 0.825 1.000 0.562 -1.000 -1.000 ## [61] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.938 ## [71] 0.540 1.000 0.778 -0.715 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [81] -1.000 -1.000 -1.000 -1.000 -1.000 0.100 1.000 0.922 -0.439 -1.000 ## [91] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [101] -0.257 0.950 1.000 -0.162 -1.000 -1.000 -1.000 -0.987 -0.714 -0.832 ## [111] -1.000 -1.000 -1.000 -1.000 -1.000 -0.797 0.909 1.000 0.300 -0.961 ## [121] -1.000 -1.000 -0.550 0.485 0.996 0.867 0.092 -1.000 -1.000 -1.000 ## [131] -1.000 0.278 1.000 0.877 -0.824 -1.000 -0.905 0.145 0.977 1.000 ## [141] 1.000 1.000 0.990 -0.745 -1.000 -1.000 -0.950 0.847 1.000 0.327 ## [151] -1.000 -1.000 0.355 1.000 0.655 -0.109 -0.185 1.000 0.988 -0.723 ## [161] -1.000 -1.000 -0.630 1.000 1.000 0.068 -0.925 0.113 0.960 0.308 ## [171] -0.884 -1.000 -0.075 1.000 0.641 -0.995 -1.000 -1.000 -0.677 1.000 ## [181] 1.000 0.753 0.341 1.000 0.707 -0.942 -1.000 -1.000 0.545 1.000 ## [191] 0.027 -1.000 -1.000 -1.000 -0.903 0.792 1.000 1.000 1.000 1.000 ## [201] 0.536 0.184 0.812 0.837 0.978 0.864 -0.630 -1.000 -1.000 -1.000 ## [211] -1.000 -0.452 0.828 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ## [221] 1.000 0.135 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.483 0.813 ## [231] 1.000 1.000 1.000 1.000 1.000 1.000 0.219 -0.943 -1.000 -1.000 ## [241] -1.000 -1.000 -1.000 -1.000 -1.000 -0.974 -0.429 0.304 0.823 1.000 ## [251] 0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.000 Un enfoque más utilizado anteriormente para resolver este tipo de problemas consistía en procesar estas imágenes con filtros hechos a mano (por ejemplo, calcular cuántos pixeles están prendidos, si existen ciertas curvas o trazos) para después construir reglas para determinar cada dígito. Actualmente, el enfoque más exitoso es utilizar métodos de aprendizaje que aprendan automáticamente esos filtros y esas reglas basadas en filtros (redes convolucionales). Ejemplo: predecir ingreso trimestral Consideramos la medición de ingreso total trimestral para una muestra de hogares de la encuesta de ENIGH. Cada una de estas mediciones es muy costosa en tiempo y dinero. dat_ingreso &lt;- read_csv(file = &#39;datos/enigh-ejemplo.csv&#39;) ggplot(dat_ingreso, aes(x=INGTOT)) + geom_histogram(bins = 100) + scale_x_log10() Pero quizá podemos usar otras variables más fácilmente medibles para predecir el ingreso de un hogar. Por ejemplo, si consideramos el número de focos en la vivienda: ggplot(dat_ingreso, aes(x = FOCOS, y = INGTOT)) + geom_point() + scale_y_log10() + xlim(c(0,50)) O el tamaño de la localidad: ggplot(dat_ingreso, aes(x = tamaño_localidad, y = INGTOT)) + geom_boxplot() + scale_y_log10() En algunas encuestas se pregunta directamente el ingreso mensual del hogar. La respuesta es generalmente una mala estimación del verdadero ingreso, por lo que actualmente se prefiere utilizar aprendizaje para estimar a partir de otras variables que son más fielmente reportadas por encuestados (años de estudio, ocupación, número de focos en el hogar, etc.) Aprendizaje supervisado Las tareas de aprendizaje se divide en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado. Aprendizaje supervisado Construir un modelo o algoritmo para predecir o estimar un target o una variable de salida a partir de ciertas variables de entrada. Predecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno. Por ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago. Usualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida: Problemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión Problemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación. Ejemplo: predecir el rendimiento de un coche. Estimar directamente el rendimiento (km por litro de combustible) de un coche es costoso: hay que hacer varias pruebas en diversas condiciones, etc. ¿Podríamos estimar el rendimiento de un coche usando variables más accesibles, peso del coche, año de producción, etc.? library(ISLR) datos &lt;- Auto[, c(&#39;name&#39;, &#39;weight&#39;,&#39;year&#39;, &#39;mpg&#39;)] datos$peso_kg &lt;- datos$weight*0.45359237 datos$rendimiento_kpl &lt;- datos$mpg*(1.609344/3.78541178) set.seed(213) datos_muestra &lt;- sample_n(datos, 50) datos_muestra %&gt;% select(name, peso_kg, rendimiento_kpl) ## name peso_kg rendimiento_kpl ## 9 pontiac catalina 2007.1462 5.952012 ## 139 dodge coronet custom (sw) 2021.6612 5.952012 ## 248 datsun b210 gx 938.9362 16.750662 ## 229 ford granada 1598.9131 7.865159 ## 166 chevrolet monza 2+2 1461.0210 8.502874 ## 321 datsun 510 hatchback 1104.0438 15.730317 ## 5 ford torino 1564.4401 7.227443 ## 145 toyota corona 747.9738 13.179455 ## 282 mercury zephyr 6 1356.2412 8.417845 ## 297 amc spirit dl 1211.0916 11.648938 ## 19 datsun pl510 966.1517 11.478880 ## 320 mazda 626 1153.0318 13.306998 ## 218 buick opel isuzu deluxe 977.4916 12.754311 ## 1 chevrolet chevelle malibu 1589.3877 7.652587 ## 195 amc hornet 1399.3325 9.565733 ## 317 dodge aspen 1533.5958 8.120245 ## 35 plymouth satellite custom 1559.9042 6.802299 ## 356 honda prelude 1002.4391 14.327343 ## 250 oldsmobile cutlass salon brougham 1526.3383 8.460360 ## 373 pontiac phoenix 1240.5751 11.478880 ## 80 renault 12 (sw) 992.9137 11.053736 ## 201 ford granada ghia 1621.1391 7.652587 ## 202 pontiac ventura sj 1653.3442 7.865159 ## 59 dodge colt hardtop 964.3374 10.628593 ## 277 saab 99gle 1267.7907 9.183104 ## 108 amc gremlin 1265.0691 7.652587 ## 329 mercedes-benz 240d 1474.1752 12.754311 ## 220 plymouth arrow gs 1043.2625 10.841165 ## 209 plymouth volare premier v8 1787.1539 5.526868 ## 263 chevrolet monte carlo landau 1553.5539 8.162759 ## 178 audi 100ls 1221.9778 9.778305 ## 182 honda civic cvcc 814.1983 14.029742 ## 16 plymouth duster 1285.0272 9.353162 ## 191 ford gran torino 1911.8918 6.164584 ## 113 ford pinto 1047.7984 8.077730 ## 285 dodge aspen 6 1524.0704 8.757960 ## 49 ford mustang 1423.8264 7.652587 ## 243 bmw 320i 1179.3402 9.140590 ## 271 toyota celica gt liftback 1140.7848 8.970532 ## 349 toyota tercel 929.8644 16.027918 ## 339 plymouth reliant 1129.4450 11.563909 ## 309 pontiac phoenix 1159.3821 14.242314 ## 345 plymouth champ 850.4857 16.580605 ## 91 mercury marquis brougham 2246.1894 5.101724 ## 275 audi 5000 1283.6664 8.630417 ## 46 amc hornet sportabout (sw) 1343.5406 7.652587 ## 255 ford fairmont (auto) 1344.9014 8.587903 ## 7 chevrolet impala 1974.9412 5.952012 ## 378 plymouth horizon miser 963.8838 16.155461 ## 6 ford galaxie 500 1969.0445 6.377156 Y podríamos comenzar graficando rendimiento contra peso. Cada punto representa un coche distinto. En esta gráfica vemos que los valores de rendimiento varían según según peso de una manera sistemática: cuanto más grande es el peso, más bajo es el rendimiento: library(ggplot2) ggplot(datos_muestra, aes(x=peso_kg, y=rendimiento_kpl)) + geom_point() Podemos entonces ajustar una curva, que para cada nivel de peso da un valor de rendimiento que se ‘aleja lo menos posible’ de los valores de rendimiento cercanos. Por ejemplo: según la curva roja, ¿cómo haríamos la predicción para un peso de 1500 kg? ggplot(datos_muestra, aes(x=peso_kg, y=rendimiento_kpl)) + geom_point() + geom_smooth(se =FALSE, colour=&#39;red&#39;, size=1.1, span=0.4, method=&#39;loess&#39;) + geom_smooth(se =FALSE, colour=&#39;gray&#39;, size=1.1, span=2, method=&#39;loess&#39;) Aprendizaje no supervisado Aprendizaje no supervisado En este caso no hay target o variable salida. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos. Los problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles. Ejemplo: tipos de coches en el mercado Quisieramos encontrar categorías de coches tales que: las categorías son diferentes entre sí, y los coches en una misma categoría son similares entre sí. Esta agrupación nos permite entender la estructura general de los datos, cómo están organizados en términos de similitud de características. En este ejemplo, encontramos un plano de máxima variabilidad donde proyectamos los coches, y después formamos grupos de coches similares: autos &lt;- Auto %&gt;% select(mpg, displacement, horsepower, acceleration) comps_autos &lt;- princomp(autos, cor = TRUE) clust &lt;- hclust(dist(comps_autos$scores[,1:2]), method = &#39;ward.D&#39;) autos$grupo &lt;- cutree(clust, k = 4) autos$Comp.1 &lt;- comps_autos$scores[,1] autos$Comp.2 &lt;- comps_autos$scores[,2] autos$nombre &lt;- Auto$name ggplot(autos, aes(x=Comp.1, y=Comp.2, colour=factor(grupo), label=nombre)) + geom_point() ¿Cómo interpretamos los grupos? head(filter(autos, grupo==1)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 18 307 130 12.0 1 -1.817719 -0.5042535 ## 2 15 350 165 11.5 1 -2.800712 -0.3938195 ## 3 18 318 150 11.0 1 -2.310357 -0.7966085 ## 4 16 304 150 12.0 1 -2.213807 -0.3989781 ## 5 17 302 140 10.5 1 -2.225309 -0.9183779 ## 6 15 429 198 10.0 1 -3.900596 -0.6915313 ## nombre ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 head(filter(autos, grupo==3)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 22 198 95 15.5 3 0.01913364 0.090471378 ## 2 18 199 97 15.5 3 -0.26705470 0.339015545 ## 3 21 200 85 16.0 3 0.16412490 0.315611651 ## 4 21 199 90 15.0 3 -0.05362631 0.004579963 ## 5 19 232 100 13.0 3 -0.79359758 -0.413938751 ## 6 16 225 105 15.5 3 -0.63973365 0.517394423 ## nombre ## 1 plymouth duster ## 2 amc hornet ## 3 ford maverick ## 4 amc gremlin ## 5 amc gremlin ## 6 plymouth satellite custom head(filter(autos, grupo==2)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 24 113 95 15.0 2 0.50234800 -0.3800473 ## 2 27 97 88 14.5 2 0.79722704 -0.7509781 ## 3 24 107 90 14.5 2 0.52837050 -0.5437610 ## 4 26 121 113 12.5 2 -0.04757934 -1.2605758 ## 5 27 97 88 14.5 2 0.79722704 -0.7509781 ## 6 28 140 90 15.5 2 0.76454526 -0.4100595 ## nombre ## 1 toyota corona mark ii ## 2 datsun pl510 ## 3 audi 100 ls ## 4 bmw 2002 ## 5 datsun pl510 ## 6 chevrolet vega 2300 head(filter(autos, grupo==4)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 26 97 46 20.5 4 2.2421696 1.1703377 ## 2 25 110 87 17.5 4 1.0737328 0.3205227 ## 3 25 104 95 17.5 4 0.9902507 0.3021997 ## 4 22 140 72 19.0 4 1.1727317 1.0419917 ## 5 30 79 70 19.5 4 2.0927389 0.5620939 ## 6 31 71 65 19.0 4 2.1920905 0.3319627 ## nombre ## 1 volkswagen 1131 deluxe sedan ## 2 peugeot 504 ## 3 saab 99e ## 4 chevrolet vega (sw) ## 5 peugeot 304 ## 6 toyota corolla 1200 1.2 Aprendizaje Supervisado Por el momento nos concentramos en problemas supervisados de regresión, es decir predicción de variables numéricas. ¿Cómo entendemos el problema de predicción? 1.2.1 Proceso generador de datos (modelo teórico) Para entender lo que estamos intentando hacer, pensaremos en términos de modelos probabilísticos que generan los datos. La idea es que estos representan los procesos que generan los datos o las observaciones. Si \\(Y\\) es la respuesta que queremos predecir, y \\(X\\) es una entrada que queremos usar para predecir \\(Y\\), consideramos que las variables aleatorias \\(Y\\) y \\(X\\) están relacionadas como sigue: \\[Y=f(X)+\\epsilon,\\] donde \\(\\epsilon\\) es una término de error aleatorio que no depende de \\(X\\), y que tiene valor esperado \\(\\textrm{E}(\\epsilon)=0\\). \\(f\\) expresa la relación sistemática que hay entre \\(Y\\) y \\(X\\): para cada valor posible de \\(X\\), la contribución de \\(X\\) a \\(Y\\) es \\(f(X)\\). Pero \\(X\\) no determina a \\(Y\\), como en el ejemplo anterior de rendimiento de coches. Entonces agregamos una error aleatorio \\(\\epsilon\\), con media cero (si la media no es cero podemos agregar una constante a \\(f\\)), que no contiene información acerca de \\(X\\) (independiente de \\(X\\)). \\(\\epsilon\\) representa, por ejemplo, el efecto de variables que no hemos medido o procesos aleatorios que determinan la respuesta. Ejemplo Vamos a usar simulación para entender estas ideas: supongamos que \\(X\\) es el número de años de estudio de una persona y \\(Y\\) es su ingreso mensual. En primer lugar, estas son el número de años de estudio de 8 personas: x &lt;- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2) Ahora supondremos que la dependencia de Y de X está dada por \\(Y=f(X)+\\epsilon\\) por una función \\(f\\) que no conocemos (esta función está determinada por el fenómeno) f &lt;- function(x){ ifelse(x &lt; 10, 1000*sqrt(x), 1000*sqrt(10)) } El ingreso no se determina únicamente por número de años de estudio. Suponemos entonces que hay algunas variables adicional que perturba los niveles de \\(f(X)\\) por una cantidad aleatoria. Los valores que observamos de \\(Y\\) están dados entonces por \\(Y=f(X)+\\epsilon\\). Entonces podríamos obtener, por ejemplo: x_g &lt;- seq(0,20,0.5) y_g &lt;- f(x_g) dat_g &lt;- data.frame(x = x_g, y = y_g) set.seed(281) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos &lt;- data_frame(x = x, y = y) datos$y_media &lt;- f(datos$x) ggplot(datos, aes(x = x, y = y)) + geom_point() + geom_line(data=dat_g, colour = &#39;blue&#39;, size = 1.1) + geom_segment(aes(x = x, xend = x, y = y, yend = y_media), col=&#39;red&#39;) En problemas de aprendizaje nunca conocemos esta \\(f\\) verdadera, aunque quizá sabemos algo acerca de sus propiedades (por ejemplo, continua, de variación suave). Lo que tenemos son los datos, que también podrían haber resultado en (para otra muestra de personas, por ejemplo): set.seed(28015) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos &lt;- data.frame(x = x, y = y) ggplot(datos, aes(x = x, y = y)) + geom_point() La siguiente observación nos da una idea de lo que intentamos hacer, aunque todavía es vaga y requiere refinamiento: Bajo los supuestos del modelo Y = f(X)+ϵ, aprender de los datos significa intentar recuperar o estimar la forma de la función f que no conocemos. f representa la relación sistemática entre Y y X. ¿Qué tan bien podemos estimar esa \\(f\\) que no conocemos, con los datos disponibles? ¿Qué significa estimar bien? Incluso este ejemplo tan simple muestra las dificultades que vamos a enfrentar, y la importancia de determinar con cuidado qué tanta información tenemos, y qué tan buenas pueden ser nuestras predicciones. 1.3 Predicciones La idea es entonces producir una estimación de f que nos permita hacer predicciones. Si denotamos por \\(\\hat{f}\\) a una estimación de \\(f\\) construida a partir de los datos, podemos hacer predicciones aplicando \\(\\hat{f}\\) a valores de \\(X\\). La predicción de Y la denotamos por \\(\\hat{Y}\\), y \\[\\hat{Y}=\\hat{f}(X).\\] El error de predicción (residual) está dado por el valor observado menos la predicción: \\[Y-\\hat{Y}.\\] En nuestro ejemplo anterior, podríamos construir, por ejemplo, una recta ajustada por mínimos cuadrados: curva_1 &lt;- geom_smooth(data=datos, method = &quot;lm&quot;, se=FALSE, color=&quot;red&quot;, formula = y ~ x, size = 1.1) ggplot(datos, aes(x = x, y = y)) + geom_point() + curva_1 En este caso \\(\\hat{f}\\) es una recta, y la podemos usar para hacer predicciones. Por ejemplo, si tenemos una observación con \\(x_0=8\\) años de estudio, nuestra predicción del ingreso \\(\\hat{y}=\\hat{f}(8)\\) sería lineal &lt;- lm(y ~ x,data = datos) pred_1 &lt;- predict(lineal, newdata = data.frame(x=8)) pred_1 ## 1 ## 2193.561 ggplot(datos, aes(x = x, y = y)) + geom_point() + curva_1 + geom_segment(x = 0, xend = 8, y = pred_1, yend = pred_1, colour = &#39;salmon&#39;) + geom_segment(x = 8, xend = 8, y = 0, yend = pred_1, colour = &#39;salmon&#39;) + annotate(&#39;text&#39;, x = 0.5, y = pred_1 + 100, label = round(pred_1, 1)) + geom_point( x= 8, y =3200, col=&#39;green&#39;, size = 4) Si observamos que para esta observación con \\(x_0=8\\), resulta que el correspondiente ingreso es \\(y_0=3200\\), entonces el error sería y_0 &lt;- 3200 y_0 - pred_1 ## 1 ## 1006.439 En aprendizaje buscamos que estos errores sean lo más cercano a cero que sea posible. 1.4 Cuantificación de error o precisión El elemento faltante para definir la tarea de aprendizaje supervisado es qué significa aproximar bien a \\(f\\), o tener predicciones precisas. Para esto definimos una función de pérdida: \\[L(Y, \\hat{f}(X)),\\] que nos dice cuánto nos cuesta hacer la predicción \\(\\hat{f}(X)\\) cuando el verdadero valor es \\(Y\\) y las variables de entrada son \\(X\\). Una opción conveniente para problemas de regresión es la pérdida cuadrática: \\[L(Y, \\hat{f}(X)) = (Y - \\hat{f}(X))^2\\] Esta es una cantidad aleatoria, de modo que en algunos casos este error puede ser más grande o más chico. Usualmente buscamos una \\(\\hat{f}\\) de modo que el error promedio sea chico: \\[Err = E (Y - \\hat{f}(X))^2 \\] Nota: Intenta demostrar que bajo error cuadrático medio y suponiendo el modelo aditivo \\(Y=f(X)+\\epsilon\\), el mejor predictor de \\(Y\\) es \\(f(x)= E[Y|X=x]\\). Es decir: lo que nos interesa es aproximar lo mejor que se pueda la esperanza condicional 1.5 Tarea de aprendizaje supervisado Ahora tenemos los elementos para definir con precisión el problema de aprendizaje supervisado. Consideramos un proceso generador de datos \\((X,Y)\\). En primer lugar, tenemos datos de los que vamos a aprender. Supongamos que tenemos un conjunto de datos etiquetados (generados según \\((X,Y)\\)) \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] que llamamos conjunto de entrenamiento. Nótese que usamos minúsculas para denotar observaciones particulares de \\((X,Y)\\). Un algoritmo de aprendizaje es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\): \\[{\\mathcal L} \\to \\hat{f}.\\] El desempeño del predictor particular \\(\\hat{f}\\) se mide como sigue: si en el futuro observamos otra muestra \\({\\mathcal T}\\) (que podemos llamar muestra de prueba) \\[{\\mathcal T}=\\{ (x_0^{(1)},y_0^{(1)}),(x_0^{(2)},y_0^{(2)}), \\ldots, (x_0^{(m)}, y_0^{(m)}) \\}\\] entonces decimos que el error de predicción (cuadrático) de \\(\\hat{f}\\) para el ejemplo \\((x_0^{(j)},y_0^{(j)})\\) está dado por \\[(y_0^{(j)} - \\hat{f}(x_0^{(j)}))^2\\] y el error sobre la muestra \\({\\mathcal T}\\) es \\[\\hat{Err} = \\frac{1}{m}\\sum_{j=1}^m (y_0^{(j)} - \\hat{f}(x_0^{(j)}))^2\\] Es muy importante considerar dos muestras separadas en esta definición: No tiene mucho sentido medir el desempeño de nuestro algoritmo sobre la muestra de entrenamiento, pues el algoritmo puede ver las etiquetas. Considerar el error sobre una muestra diferente a la de entrenamiento nos permite evaluar si nuestro algoritmo generaliza, que se puede pensar como “verdadero” aprendizaje. Nótese que \\(\\hat{Err}\\) es una estimación de \\(Err\\) (por la ley de los grandes números, si \\({\\mathcal T}\\) es muestra i.i.d. de \\((X,Y)\\)). También consideramos el error de entrenamiento, dado por \\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - \\hat{f}(x^{(i)}))^2\\] - Pregunta: ¿Por qué \\(\\overline{err}\\) no necesariamente es una buena estimación de \\(Err\\)? 1.5.0.1 Ejemplo En el ejemplo que hemos estado usando, ¿que curva preferirías para predecir, la gris, la roja o la azul? ¿Cuál tiene menor error de entrenamiento? set.seed(280572) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos_entrena &lt;- data.frame(x=x, y=y) head(datos_entrena) ## x y ## 1 1 86.22033 ## 2 7 2353.75863 ## 3 10 3078.71029 ## 4 0 -397.80229 ## 5 0 424.73363 ## 6 5 3075.92998 curva.1 &lt;- geom_smooth(data=datos_entrena, method = &quot;loess&quot;, se=FALSE, color=&quot;gray&quot;, span=1, size=1.1) curva.2 &lt;- geom_smooth(data=datos_entrena, method = &quot;loess&quot;, se=FALSE, color=&quot;red&quot;, span=0.3, size=1.1) curva.3 &lt;- geom_smooth(data=datos_entrena, method = &quot;lm&quot;, se=FALSE, color=&quot;blue&quot;, size=1.1) ggplot(datos_entrena, aes(x=x, y=y)) + geom_point() + curva.1 + curva.2 + curva.3 Calculamos los errores de entrenamiento de cada curva: mod_rojo &lt;- loess(y ~ x, data = datos_entrena, span=0.3) mod_gris &lt;- loess(y ~ x, data = datos_entrena, span=1) mod_recta &lt;- lm(y ~ x, data = datos_entrena) df_mods &lt;- data_frame(nombre = c(&#39;recta&#39;, &#39;rojo&#39;,&#39;gris&#39;)) df_mods$modelo &lt;- list(mod_recta, mod_rojo, mod_gris) error_f &lt;- function(df){ function(mod){ preds &lt;- predict(mod, newdata = df) round(sqrt(mean((preds-df$y)^2))) } } error_ent &lt;- error_f(datos_entrena) df_mods &lt;- df_mods %&gt;% mutate(error_entrena = map_dbl(modelo, error_ent)) df_mods ## # A tibble: 3 x 3 ## nombre modelo error_entrena ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 recta &lt;S3: lm&gt; 782 ## 2 rojo &lt;S3: loess&gt; 189 ## 3 gris &lt;S3: loess&gt; 389 El error de entrenamiento es considerablemente menor para la curva roja, y es más grande para la recta. Sin embargo, consideremos que tenemos una nueva muestra (de prueba). set.seed(218052272) x_0 &lt;- sample(0:13, 100, replace = T) error &lt;- rnorm(length(x_0), 0, 500) y_0 &lt;- f(x_0) + error datos_prueba &lt;- data_frame(x = x_0, y = y_0) datos_prueba ## # A tibble: 100 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 9 2156.1160 ## 2 11 3227.0968 ## 3 3 2382.2805 ## 4 10 3481.5850 ## 5 7 2732.8020 ## 6 7 2325.9217 ## 7 12 3463.7795 ## 8 0 -563.9233 ## 9 10 3295.5705 ## 10 0 365.9880 ## # ... with 90 more rows error_p &lt;- error_f(datos_prueba) df_mods &lt;- df_mods %&gt;% mutate(error_prueba = map_dbl(modelo, error_p)) df_mods ## # A tibble: 3 x 4 ## nombre modelo error_entrena error_prueba ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 recta &lt;S3: lm&gt; 782 801 ## 2 rojo &lt;S3: loess&gt; 189 628 ## 3 gris &lt;S3: loess&gt; 389 520 1.5.1 Observaciones El mejor modelo entrenamiento es uno que “sobreajusta” a los datos, pero es el peor con una muestra de prueba. La curva roja aprende del una componente de ruido del modelo - lo cual realmente no es aprendizaje. El modelo de la recta no es bueno en entrenamiento ni en prueba. Este modelo no tiene la capacidad para aprender de la señal en los datos. El mejor modelo en la muestra de prueba es uno que está entre la recta y la curva roja en términos de flexibilidad. Nuestra intuición para escoger el modelo gris desde el principio se refleja en que generaliza mejor que los otros, y eso a su vez se refleja en un error de prueba más bajo. 1.6 ¿Por qué tenemos errores? ¿De dónde provienen los errores en la predicción? Si establemos que el error es una función creciente de \\(Y-\\hat{Y}\\), vemos que \\[ Y-\\hat{Y} = f(X) + \\epsilon - \\hat{f}(X)= (f(X) - \\hat{f}(X)) + \\epsilon,\\] donde vemos que hay dos componentes que pueden hacer grande a \\(Y-\\hat{Y}\\): La diferencia \\(f(X) - \\hat{f}(X)\\) está asociada a error reducible, pues depende de qué tan bien estimemos \\(f(X)\\) con \\(\\hat{f}(X)\\) El error aleatorio \\(\\epsilon\\), asociado a error irreducible. Cualquiera de estas dos cantidades pueden hacer que nuestras predicciones no sean precisas. En nuestro ejemplo anterior, el error reducible: Es grande para el modelo rojo, pues responde demasiado fuerte a ruido en los datos (tiene varianza alta). Es grande para el modelo de la recta, pues no tiene capacidad para acercarse a la verdadera curva (está sesgado). En aprendizaje supervisado, nuestro objetivo es reducir el error reducible tanto como sea posible (obtener la mejor estimación de f). No podemos hacer nada acerca del error irreducible, pues este se debe a aleatoriedad en el fenómeno o a variables que no conocemos. Notación Las observaciones o datos que usaremos para construir nuestras estimaciones las denotamos como sigue. Cada {} (o caso, o ejemplo) está dada por un el valor de una variable de entrada \\(X\\) y un valor de la variable de salida \\(Y\\). Cuando tenemos \\(N\\) ejemplos de entrenamiento, los escribimos como los pares \\[(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}) \\ldots, (x^{(N)},y^{(N)})\\]. Cuando los datos de entrada contienen \\(p\\) variables o atributos, escribimos \\[x^{(i)} = (x_1^{(i)}, x_2^{(i)},\\ldots, x_p^{(i)})\\] Escribimos también la matriz de entradas de dimensión Nxp: \\[\\underline{X} = \\left ( \\begin{array}{cccc} x_1^{(1)} &amp; x_2^{(1)} &amp; \\ldots &amp; x_p^{(1)} \\\\ x_1^{(2)} &amp; x_2^{(2)} &amp; \\ldots &amp; x_p^{(2)}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ x_1^{(N)} &amp; x_2^{(N)} &amp; \\ldots &amp; x_p^{(N)} \\\\ \\end{array} \\right)\\] y \\[\\underline{y} =(y^{(1)},y^{(2)}, \\ldots, y^{(N)})^t.\\] Adicionalmente, usamos la notación \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\ldots, (x^{(N)},y^{(N)}) \\}\\] para denotar al conjunto de datos con los que construimos nuestro modelo. A este conjunto le llamaremos {} (learning set) 1.7 ¿Cómo estimar f? Los métodos para construir la estimación \\(\\hat{f}\\) se dividen en dos grandes grupos: paramétricos y no paramétricos. 1.7.0.1 Métodos paramétricos En los métodos paramétricos seleccionamos, usando los datos, una \\(\\hat{f}\\) de una colección de modelos que pueden ser descritos por un número fijo de parámetros. Por ejemplo, podríamos establecer que la función \\(f\\) tiene la forma: \\[f(x_1,x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2,\\] que son funciones lineales en dos variables. En este caso, tenemos tres parámetros \\((\\beta_0,\\beta_1,\\beta_2)\\), que describen a la familia completa. Usando los datos de entrenamiento, entrenamos este modelo para encontrar \\((\\beta_0,\\beta_1,\\beta_2)\\) tales que \\[y^{(i)} \\approx \\beta_0 + \\beta_1 x_1^{(i)} + \\beta_2 x_2^{(i)},\\] es decir, el modelo ajustado regresa valores cercanos a los observados. Esta aproximación se traducirá en un problema de optimización. En general, este enfoque es muy tratable numéricamente pues el problema se reduce a estimar tres valores numéricos, en lugar de intentar estimar una función \\(f\\) arbitraria. Su desventaja es que quizá ningún miembro familia de modelos establecida (por ejemplo, modelos lineales) puede aproximar razonablemente bien a la verdadera función \\(f\\). Es decir, estos métodos tienen {} potencial grande. Por ejemplo: mod_1 &lt;- lm(mpg~year+weight, data=Auto) Auto$fit &lt;- predict(mod_1) weight &lt;- seq(1600, 5200, by=100) year &lt;- seq(70,82, by=1) z &lt;- outer(weight, year, function(x,y){ predict(mod_1, newdata=data.frame(weight=x,year=y))}) layout(t(1:2)) res &lt;- persp(weight, year, z, theta=50,phi=10, col=&#39;red&#39;, lty=1) points(trans3d(x=Auto[,&#39;weight&#39;],y=Auto[,&#39;year&#39;],z= Auto$mpg, pmat=res), col=2-(residuals(mod_1)&gt;0), cex=0.5,pch=16) res &lt;- persp(weight, year, z, theta=-20,phi=10, col=&#39;red&#39;, lty=1) points(trans3d(x=Auto[,&#39;weight&#39;],y=Auto[,&#39;year&#39;],z= Auto$mpg, pmat=res), col=2-(residuals(mod_1)&gt;0), cex=0.5,pch=16) dat.grid &lt;- expand.grid(weight=weight, year = year) values &lt;- predict(mod_1, dat.grid) dat.grid$pred.mpg &lt;- values ggplot(dat.grid, aes(x=weight, y=pred.mpg)) + facet_wrap(~year,nrow=2)+ geom_point(data=Auto, aes(x=weight, y=mpg),alpha=0.5)+ geom_line(colour=&#39;red&#39;,size=1.1) + geom_hline(yintercept=30, col=&#39;gray&#39;) Por otro lado, si la estructura rígida de estos modelos describe aproximadamente el comportamiento de los datos, estos modelos nos pueden proteger contra sobreajustar los datos, es decir, incorporar en nuestra estimación de \\(\\hat{f}\\) aspectos del error (\\(\\epsilon\\)), lo que tiene como consecuencia también predicciones pobres. Métodos no paramétricos Los métodos no paramétricos suponen menos acerca de la forma funcional de \\(f\\), y su número de parámetros depende del tamaño de los datos que estamos considerando. Potencialmente, estos métodos pueden aproximar formas funcionales mucho más generales, pero típicamente requieren de más datos para obtener resultados razonables. 1.7.0.2 Ejemplo En este ejemplo usamos regresión loess, que funciona haciendo promedios locales ponderados de puntos de entrenamiento cercanos a donde queremos hacer predicciones. La ponderación se hace en función de la distancia de los puntos de entrenamiento al punto donde queremos predecir. mod_2 &lt;- loess(mpg~year+weight, data=Auto, family=&#39;symmetric&#39;, degree=1, span=0.15) Auto$fit &lt;- predict(mod_2) weight &lt;- seq(1600, 5200, by=100) year &lt;- seq(70,82, by=1) dat.grid &lt;- expand.grid(weight = weight, year = year) values &lt;- predict(mod_2, dat.grid) %&gt;% data.frame %&gt;% gather(year, value, year.70:year.82) dat.grid$pred.mpg &lt;- values$value ggplot(dat.grid, aes(x=weight, y=pred.mpg)) + facet_wrap(~year,nrow=2)+ geom_point(data=Auto, aes(x=weight, y=mpg),alpha=0.5)+ geom_line(colour=&#39;red&#39;,size=1.1) + geom_hline(yintercept=30, col=&#39;gray&#39;) 1.8 Resumen Aprendizaje de máquina: algoritmos que aprenden de los datos para predecir cantidades numéricas, o clasificar (aprendizaje supervisado), o para encontrar estructura en los datos (aprendizaje no supervisado). En aprendizaje supervisado, el esquema general es: un algoritmo aprende de una muestra de entrenamiento \\({\\mathcal L}\\), que es generada por el proceso generador de datos que nos interesa. Eso quiere decir que produce una función \\(\\hat{f}\\) (a partir de \\({\\mathcal L}\\)) que nos sirve para hacer predicciones \\(x \\to \\hat{f}(x)\\) de \\(y\\) El error de predicción del algoritmo es \\(Err\\), que mide en promedio qué tan lejos están las predicciones de valores reales. Para estimar esta cantidad usamos una muestra de prueba \\({\\mathcal T}\\), que es independiente de \\({\\mathcal L}\\). Esta es porque nos interesa el desempeño futuro de \\(\\hat{f}\\) para nuevos casos que el algoritmo no ha visto (esto es aprender). El error en la muestra de entrenamiento no necesariamente es buen indicador del desempeño futuro de nuestro algoritmo. Para obtener las mejores predicciones posibles, es necesario que el algoritmo sea capaz de capturar patrones en los datos, pero no tanto que tienda a absorber ruido en la estimación - es un balance de complejidad y rigidez. En términos estadísticos, se trata de un balance de varianza y sesgo. Hay dos tipos de métodos generales: paramétricos y no paramétricos. Los paramétricos seleccionan un número finito de parámetros para construir \\(\\hat{f}\\), el número de parámetros de los no paramétricos depende del conjunto de entrenamiento. 1.9 Tarea En el ejemplo simple que vimos en la sección 1.5, utilizamos una sola muestra de entrenamiento para evaluar el algoritmo. ¿Será posible que escogimos una muestra atípica? - Corre el ejemplo con otra muestra y reporta tus resultados de error de entrenamiento y error de prueba para los tres métodos. - Opcional (difícil): evalúa los tres métodos comparando estos valores para un número grande de distintas simulaciones de los datos de entrenamiento. "],
["regresion.html", "Clase 2 Regresión lineal 2.1 Introducción 2.2 Aprendizaje de coeficientes (ajuste) 2.3 Descenso en gradiente 2.4 Descenso en gradiente para regresión lineal 2.5 Normalización de entradas 2.6 Interpretación de modelos lineales 2.7 Solución analítica 2.8 ¿Por qué el modelo lineal funciona bien (muchas veces)?", " Clase 2 Regresión lineal 2.1 Introducción Consideramos un problema de regresión con entradas \\(X=(X_1,X_2,\\ldots, X_p)\\) y salida \\(Y\\). Una de las maneras más simples que podemos intentar para predecir \\(Y\\) en función de las \\(X_j\\)´s es mediante una suma ponderada de los valores de las \\(X_j&#39;s\\), usando una función \\[f_\\beta (X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\] Nuestro trabajo será entonces, dada una muestra de entrenamiento \\({\\mathcal L}\\), encontrar valores apropiados de las \\(\\beta\\)’s, para construir un predictor: \\[\\hat{f}(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 \\cdots + \\hat{\\beta} X_p\\] y usaremos esta función \\(\\hat{f}\\) para hacer predicciones \\(\\hat{Y} =\\hat{f}(X)\\). 2.1.0.1 Ejemplos Queremos predecir las ventas futuras anuales \\(Y\\) de un supermercado que se va a construir en un lugar dado. Las variables que describen el lugar son \\(X_1 = trafico\\_coches\\), \\(X_2=trafico\\_peatones\\). En una aproximación simple, podemos suponer que la tienda va a capturar una fracción de esos tráficos que se van a convertir en ventas. Quisieramos predecir con una función de la forma \\[f_\\beta (peatones, coches) = \\beta_0 + \\beta_1\\, peatones + \\beta_2\\, coches.\\] Por ejemplo, después de un análisis estimamos que \\(\\hat{\\beta}_0 = 1000000\\) (ventas base) \\(\\hat{\\beta}_1 = (200)*0.02 = 4\\) \\(\\hat{\\beta}_2 = (300)*0.01 =3\\) Entonces haríamos predicciones con \\[\\hat{f}(peatones, coches) = 1000000 + 4\\,peatones + 3\\, coches\\] El modelo lineal es más flexible de lo que parece en una primera aproximación, porque tenemos libertad para construir las variables de entrada a partir de nuestros datos. Por ejemplo, si tenemos una tercera variable \\(estacionamiento\\) que vale 1 si hay un estacionamiento cerca o 0 si no lo hay, podríamos definir las variables \\(X_1= peatones\\) \\(X_2 = coches\\) \\(X_3 = estacionamiento\\) \\(X_4 = coches*estacionamiento\\) Donde la idea de agregar \\(X_4\\) es que si hay estacionamiento entonces vamos a capturar una fracción adicional del trafico de coches, y la idea de \\(X_3\\) es que la tienda atraerá más nuevas visitas si hay un estacionamiento cerca. Buscamos ahora modelos de la forma \\[f_\\beta(X_1,X_2,X_3,X_4) = \\beta_0 + \\beta_1X_1 + \\beta_2 X_2 + \\beta_3 X_3 +\\beta_4 X_4\\] y podríamos obtener después de nuestra análisis las estimaciones \\(\\hat{\\beta}_0 = 800000\\) (ventas base) \\(\\hat{\\beta}_1 = 4\\) \\(\\hat{\\beta}_2 = (300)*0.005 = 1.5\\) \\(\\hat{\\beta}_3 = 400000\\) \\(\\hat{\\beta}_4 = (300)*0.02 = 6\\) y entonces haríamos predicciones con el modelo \\[\\hat{f} (X_1,X_2,X_3,X_4) = 800000 + 4\\, X_1 + 1.5 \\,X_2 + 400000\\, X_3 +6\\, X_4\\] 2.2 Aprendizaje de coeficientes (ajuste) En el ejemplo anterior, los coeficientes fueron calculados (o estimados) usando experiencia, argumentos teóricos, o quizá otras fuentes de datos (como estudios o encuestas, conteos, etc.) Ahora quisiéramos construir un algoritmo para aprender estos coeficientes del modelo \\[f_\\beta (X_1) = \\beta_0 + \\beta_1 X_1 + \\cdots \\beta_p X_p\\] a partir de una muestra de entrenamiento \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] El criterio de ajuste (algoritmo de aprendizaje) más usual para regresión lineal es el de mínimos cuadrados. Construimos las predicciones (ajustados) para la muestra de entrenamiento: \\[\\hat{y}^{(i)} = f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\] Y consideramos las diferencias de los ajustados con los valores observados: \\[e^{(i)} = y^{(i)} - f_\\beta (x^{(i)})\\] La idea entonces es minimizar la suma de los residuales al cuadrado, para intentar que la función ajustada pase lo más cercana a los puntos de entrenamiento que sea posible. Si \\[RSS(\\beta) = \\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] Queremos resolver Mínimos cuadrados \\[\\min_{\\beta} RSS(\\beta) = \\min_{\\beta}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] 2.2.0.1 Ejemplo Consideremos library(readr) library(dplyr) library(knitr) prostata &lt;- read_csv(&#39;datos/prostate.csv&#39;) %&gt;% select(lcavol, lpsa, train) kable(head(prostata), format = &#39;html&#39;) lcavol lpsa train -0.5798185 -0.4307829 TRUE -0.9942523 -0.1625189 TRUE -0.5108256 -0.1625189 TRUE -1.2039728 -0.1625189 TRUE 0.7514161 0.3715636 TRUE -1.0498221 0.7654678 TRUE prostata_entrena &lt;- filter(prostata, train) ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() En este caso, buscamos ajustar el modelo (tenemos una sola entrada) \\(f_{\\beta} (X_1) = \\beta_0 + \\beta_1 X_1\\), que es una recta. Los cálculos serían como sigue: rss_calc &lt;- function(datos){ y &lt;- datos$lpsa x &lt;- datos$lcavol fun_out &lt;- function(beta){ y_hat &lt;- beta[1] + beta[2]*x e &lt;- (y - y_hat) rss &lt;- sum(e^2) 0.5*rss } fun_out } Nuestra función rss es entonces: rss &lt;- rss_calc(prostata_entrena) Por ejemplo, si consideramos \\((\\beta_0, \\beta_1) = (1, 1)\\), obtenemos beta &lt;- c(0,1.5) rss(beta) ## [1] 61.63861 Que corresponde a la recta ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = beta[2], intercept = beta[1], col =&#39;red&#39;) Podemos comparar con \\((\\beta_0, \\beta_1) = (1, 1)\\), obtenemos beta &lt;- c(1,1) rss(beta) ## [1] 27.11781 ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = beta[2], intercept = beta[1], col =&#39;red&#39;) Ahora minimizamos. Podríamos hacer res_opt &lt;- optim(c(0,0), rss, method = &#39;BFGS&#39;) beta_hat &lt;- res_opt$par beta_hat ## [1] 1.5163048 0.7126351 res_opt$convergence ## [1] 0 ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = 1, intercept = 1, col =&#39;red&#39;) + geom_abline(slope = beta_hat[2], intercept = beta_hat[1]) 2.3 Descenso en gradiente Aunque el problema de mínimos cuadrados se puede resolver analíticamente, proponemos un método numérico básico que es efectivo y puede escalarse a problemas grandes de manera relativamente simple: descenso en gradiente, o descenso máximo. Supongamos que una función \\(h(x)\\) es convexa y tiene un mínimo. La idea de descenso en gradiente es comenzar con un candidato inicial \\(z_0\\) y calcular la derivada en \\(z^{(0)}\\). Si \\(h(z^{(0)})&gt;0\\), la función es creciente en \\(z^{(0)}\\) y nos movemos ligeramente a la izquierda para obtener un nuevo candidato \\(z^{(1)}\\). si \\(h(z^{(0)})&lt;0\\), la función es decreciente en \\(z^{(0)}\\) y nos movemos ligeramente a la derecha para obtener un nuevo candidato \\(z^{(1)}\\). Iteramos este proceso hasta que la derivada es cercana a cero (estamos cerca del óptimo). Si \\(\\eta&gt;0\\) es una cantidad chica, podemos escribir \\[z^{(1)} = z^{(0)} + \\eta \\,h&#39;(z^{(0)}).\\] Nótese que cuando la derivada tiene magnitud alta, el movimiento de \\(z^{(0)}\\) a \\(z^{(1)}\\) es más grande, y siempre nos movemos una fracción de la derivada. En general hacemos \\[z^{(j+1)} = z^{(j)} + \\eta\\,h&#39;(z^{(j)})\\] para obtener una sucesión \\(z^{(0)},z^{(1)},\\ldots\\). Esperamos a que \\(z^{(j)}\\) converja para terminar la iteración. 2.3.0.1 Ejemplo Si tenemos h &lt;- function(x) x^2 + (x - 2)^2 - log(x^2 + 1) Calculamos (a mano): h_deriv &lt;- function(x) 2 * x + 2 * (x - 2) - 2*x/(x^2 + 1) Ahora iteramos con \\(\\eta = 0.4\\) y valor inicial \\(z_0=5\\) z_0 &lt;- 5 eta &lt;- 0.4 descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } z &lt;- descenso(20, 5, 0.1, h_deriv) z ## [,1] ## [1,] 5.000000 ## [2,] 3.438462 ## [3,] 2.516706 ## [4,] 1.978657 ## [5,] 1.667708 ## [6,] 1.488834 ## [7,] 1.385872 ## [8,] 1.326425 ## [9,] 1.291993 ## [10,] 1.272002 ## [11,] 1.260375 ## [12,] 1.253606 ## [13,] 1.249663 ## [14,] 1.247364 ## [15,] 1.246025 ## [16,] 1.245243 ## [17,] 1.244788 ## [18,] 1.244523 ## [19,] 1.244368 ## [20,] 1.244277 Y vemos que estamos cerca de la convergencia. curve(h, -3, 6) points(z[,1], h(z)) text(z[1:6], h(z[1:6]), pos = 3) 2.3.1 Selección de tamaño de paso \\(\\eta\\) Si hacemos \\(\\eta\\) muy chico, el algoritmo puede tardar mucho en converger: z &lt;- descenso(20, 5, 0.01, h_deriv) curve(h, -3, 6) points(z, h(z)) text(z[1:6], h(z[1:6]), pos = 3) Si hacemos \\(\\eta\\) muy grande, el algoritmo puede divergir: z &lt;- descenso(20, 5, 1.5, h_deriv) z ## [,1] ## [1,] 5.000000e+00 ## [2,] -1.842308e+01 ## [3,] 9.795302e+01 ## [4,] -4.837345e+02 ## [5,] 2.424666e+03 ## [6,] -1.211733e+04 ## [7,] 6.059265e+04 ## [8,] -3.029573e+05 ## [9,] 1.514792e+06 ## [10,] -7.573955e+06 ## [11,] 3.786978e+07 ## [12,] -1.893489e+08 ## [13,] 9.467445e+08 ## [14,] -4.733723e+09 ## [15,] 2.366861e+10 ## [16,] -1.183431e+11 ## [17,] 5.917153e+11 ## [18,] -2.958577e+12 ## [19,] 1.479288e+13 ## [20,] -7.396442e+13 Es necesario ajustar el tamaño de paso para cada problema particular. Si la convergencia es muy lenta, podemos incrementarlo. Si las iteraciones divergen, podemos disminuirlo 2.3.2 Funciones de varias variables Si ahora \\(h(z)\\) es una función de \\(p\\) variables, podemos intentar la misma idea usando el gradiente. Por cálculo sabemos que el gradiente apunta en la dirección de máximo crecimiento local. El gradiente es el vector columna con las derivadas parciales de \\(h\\): \\[\\nabla h(z) = \\left( \\frac{\\partial h}{\\partial z_1}, \\frac{\\partial h}{\\partial z_2}, \\ldots, \\frac{\\partial h}{\\partial z_p} \\right)^t\\] Y el paso de iteración, dado un valor inicial \\(z_0\\) y un tamaño de paso \\(\\eta &gt;0\\) es \\[z^{(i+1)} = z^{(i)} - \\eta \\nabla h(z^{(i)})\\] Las mismas consideraciones acerca del tamaño de paso \\(\\eta\\) aplican en el problema multivariado. h &lt;- function(z) { z[1]^2 + z[2]^2 - z[1] * z[2] } h_gr &lt;- function(z_1,z_2) apply(cbind(z_1, z_2), 1, h) grid_graf &lt;- expand.grid(z_1 = seq(-3, 3, 0.1), z_2 = seq(-3, 3, 0.1)) grid_graf &lt;- grid_graf %&gt;% mutate( val = apply(cbind(z_1,z_2), 1, h)) gr_contour &lt;- ggplot(grid_graf, aes(x = z_1, y = z_2, z = val)) + geom_contour(binwidth = 1.5, aes(colour = ..level..)) gr_contour El gradiente está dado por h_grad &lt;- function(z){ c(2*z[1] - z[2], 2*z[2] - z[1]) } Podemos graficar la dirección de máximo descenso para diversos puntos. Estas direcciones son ortogonales a la curva de nivel que pasa por cada uno de los puntos: grad_1 &lt;- h_grad(c(0,-2)) grad_2 &lt;- h_grad(c(1,1)) eta &lt;- 0.2 gr_contour + geom_segment(aes(x=0.0, xend=0.0-eta*grad_1[1], y=-2, yend=-2-eta*grad_1[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ geom_segment(aes(x=1, xend=1-eta*grad_2[1], y=1, yend=1-eta*grad_2[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ coord_fixed(ratio = 1) Y aplicamos descenso en gradiente: inicial &lt;- c(3, 1) iteraciones &lt;- descenso(20, inicial , 0.1, h_grad) iteraciones ## [,1] [,2] ## [1,] 3.0000000 1.0000000 ## [2,] 2.5000000 1.1000000 ## [3,] 2.1100000 1.1300000 ## [4,] 1.8010000 1.1150000 ## [5,] 1.5523000 1.0721000 ## [6,] 1.3490500 1.0129100 ## [7,] 1.1805310 0.9452330 ## [8,] 1.0389481 0.8742395 ## [9,] 0.9185824 0.8032864 ## [10,] 0.8151946 0.7344874 ## [11,] 0.7256044 0.6691094 ## [12,] 0.6473945 0.6078479 ## [13,] 0.5787004 0.5510178 ## [14,] 0.5180621 0.4986843 ## [15,] 0.4643181 0.4507536 ## [16,] 0.4165298 0.4070347 ## [17,] 0.3739273 0.3672807 ## [18,] 0.3358699 0.3312173 ## [19,] 0.3018177 0.2985609 ## [20,] 0.2713102 0.2690305 ggplot(data= grid_graf) + geom_contour(binwidth = 1.5, aes(x = z_1, y = z_2, z = val, colour = ..level..)) + geom_point(data = data.frame(iteraciones), aes(x=X1, y=X2), colour = &#39;red&#39;) 2.4 Descenso en gradiente para regresión lineal Vamos a escribir ahora el algoritmo de descenso en gradiente para regresión lineal. Igual que en los ejemplos anteriores, tenemos que precalcular el gradiente. Una vez que esto esté terminado, escribir la iteración es fácil. Recordamos que queremos minimizar (dividiendo entre dos para simplificar más adelante) \\[RSS(\\beta) = \\frac{1}{2}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] La derivada de la suma es la suma de las derivadas, así nos concentramos en derivar uno de los términos \\[ \\frac{1}{2}(y^{(i)} - f_\\beta(x^{(i)}))^2 \\] Usamos la regla de la cadena para obtener \\[ \\frac{1}{2}\\frac{\\partial}{\\partial \\beta_j} (y^{(i)} - f_\\beta(x^{(i)}))^2 = -(y^{(i)} - f_\\beta(x^{(i)})) \\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j}\\] Ahora recordamos que \\[f_{\\beta} (x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\] Y vemos que tenemos dos casos. Si \\(j=0\\), \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_0} = 1\\] y si \\(j=1,2,\\ldots, p\\) entonces \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j} = x_j^{(i)}\\] Entonces: \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_0} = -(y^{(i)} - f_\\beta(x^{(i)}))\\] y \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j} = - x_j^{(i)}(y^{(i)} - f_\\beta(x^{(i)}))\\] Y sumando todos los términos (uno para cada caso de entrenamiento): Gradiente para regresión lineal Sea \\(e^{(i)} = y_{(i)} - f_{\\beta} (x^{(i)})\\). Entonces \\[\\begin{equation} \\frac{\\partial RSS(\\beta)}{\\partial \\beta_0} = - \\sum_{i=1}^N e^{(i)} \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial RSS(\\beta)}{\\partial \\beta_j} = - \\sum_{i=1}^N x_j^{(i)}e^{(i)} \\tag{2.2} \\end{equation}\\] para \\(j=1,2,\\ldots, p\\). Nótese que cada punto de entrenamiento contribuye al cálculo del gradiente - la contribución es la dirección de descenso de error para ese punto particular de entrenamiento. Nos movemos entonces en una dirección promedio, para intentar hacer el error total lo más chico posible. Podemos implementar ahora estos cálculos. Aunque podríamos escribir ciclos para hacer estos cálculos, es mejor hacer los cálculos en forma matricial, de manera que aprovechamos rutinas de álgebra lineal eficiente. El cálculo del gradiente es como sigue: grad_calc &lt;- function(x_ent, y_ent){ salida_grad &lt;- function(beta){ f_beta &lt;- as.matrix(cbind(1, x_ent)) %*% beta e &lt;- y_ent - f_beta grad_out &lt;- -apply(t(cbind(1,x_ent)) %*% e, 1, sum) names(grad_out)[1] &lt;- &#39;Intercept&#39; grad_out } salida_grad } grad &lt;- grad_calc(prostata_entrena[, 1, drop = FALSE], prostata_entrena$lpsa) grad(c(0,1)) ## Intercept lcavol ## -76.30319 -70.93938 grad(c(1,1)) ## Intercept lcavol ## -9.303187 17.064556 Podemos checar nuestro cálculo del gradiente: delta &lt;- 0.001 (rss(c(1 + delta,1)) - rss(c(1,1)))/delta ## [1] -9.269687 (rss(c(1,1+delta)) - rss(c(1,1)))/delta ## [1] 17.17331 Y ahora iteramos para obtener iteraciones &lt;- descenso(30, c(0,0), 0.005, grad) iteraciones ## [,1] [,2] ## [1,] 0.0000000 0.0000000 ## [2,] 0.8215356 1.4421892 ## [3,] 0.7332652 0.9545169 ## [4,] 0.8891507 1.0360252 ## [5,] 0.9569494 0.9603012 ## [6,] 1.0353555 0.9370937 ## [7,] 1.0977074 0.9046239 ## [8,] 1.1534587 0.8800287 ## [9,] 1.2013557 0.8576489 ## [10,] 1.2430547 0.8385314 ## [11,] 1.2791967 0.8218556 ## [12,] 1.3105688 0.8074114 ## [13,] 1.3377869 0.7948709 ## [14,] 1.3614051 0.7839915 ## [15,] 1.3818983 0.7745509 ## [16,] 1.3996803 0.7663595 ## [17,] 1.4151098 0.7592518 ## [18,] 1.4284979 0.7530844 ## [19,] 1.4401148 0.7477329 ## [20,] 1.4501947 0.7430895 ## [21,] 1.4589411 0.7390604 ## [22,] 1.4665303 0.7355643 ## [23,] 1.4731155 0.7325308 ## [24,] 1.4788295 0.7298986 ## [25,] 1.4837875 0.7276146 ## [26,] 1.4880895 0.7256328 ## [27,] 1.4918224 0.7239132 ## [28,] 1.4950614 0.7224211 ## [29,] 1.4978719 0.7211265 ## [30,] 1.5003106 0.7200031 Y checamos que efectivamente el error total de entrenamiento decrece apply(iteraciones, 1, rss) ## [1] 249.60960 51.70986 32.49921 28.96515 27.22475 25.99191 25.07023 ## [8] 24.37684 23.85483 23.46181 23.16591 22.94312 22.77538 22.64910 ## [15] 22.55401 22.48242 22.42852 22.38794 22.35739 22.33438 22.31706 ## [22] 22.30402 22.29421 22.28681 22.28125 22.27706 22.27390 22.27153 ## [29] 22.26974 22.26839 Notación y forma matricial Usando la notación de la clase anterior (agregando una columna de unos al principio): \\[\\underline{X} = \\left ( \\begin{array}{ccccc} 1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \\ldots &amp; x_p^{(1)} \\\\ 1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \\ldots &amp; x_p^{(2)}\\\\ 1&amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; x_1^{(N)} &amp; x_2^{(N)} &amp; \\ldots &amp; x_p^{(N)} \\\\ \\end{array} \\right)\\] y \\[\\underline{y} =(y^{(1)},y^{(2)}, \\ldots, y^{(N)})^t.\\] Como \\[\\underline{e} = \\underline{y} - \\underline{X}\\beta\\] tenemos entonces (de las fórmulas (2.1) y (2.2)): \\[\\begin{equation} \\nabla RSS(\\beta) = \\underline{X}^t(\\underline{X}\\beta - \\underline{y}) = -\\underline{X}^t \\underline{e} \\tag{2.3} \\end{equation}\\] 2.5 Normalización de entradas La convergencia de descenso en gradiente (y también el desempeño numérico para otros algoritmos) puede dificultarse cuando las escalas tienen escalas muy diferentes. En este ejemplo simple, una variable tiene desviación estándar 10 y otra 1: x2 &lt;- rnorm(100, 0, 1) x1 &lt;- rnorm(100, 0, 10) + 5 * x2 y &lt;- 0.1 * x1 + x2 + rnorm(100, 0, 1) dat &lt;- data_frame(x1, x2, y) rss &lt;- function(beta) mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) grid_beta &lt;- expand.grid(beta1 = seq(-10, 10, 0.5), beta2 = seq(-10, 10, 0.5)) rss_1 &lt;- apply(grid_beta, 1, rss) dat_x &lt;- data.frame(grid_beta, rss_1) ggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + geom_contour() En algunas direcciones el gradiente es muy grande, y en otras chico. Esto implica que la convergencia puede ser muy lenta en algunas direcciones, puede diverger en otras, y que hay que ajustar el paso \\(\\eta &gt; 0\\) con cuidado, dependiendo de dónde comiencen las iteraciones. Una normalización usual es con la media y desviación estándar, donde hacemos, para cada variable de entrada \\(j=1,2,\\ldots, p\\) \\[ x_j^{(i)} = \\frac{ x_j^{(i)} - \\bar{x}_j}{s_j}\\] donde \\[\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^N x_j^{(i)}\\] \\[s_j = \\frac{1}{N-1}\\sum_{i=1}^N (x_j^{(i)}- \\bar{x}_j )^2\\] es decir, centramos y normalizamos por columna. Otra opción común es restar el mínimo y dividir entre la diferencia del máximo y el mínimo, de modo que las variables resultantes toman valores en \\([0,1]\\). Entonces escalamos antes de ajustar: x1_s = (x1 - mean(x1))/sd(x1) x2_s = (x2 - mean(x2))/sd(x2) dat &lt;- data_frame(x1_s, x2_s, y) rss &lt;- function(beta) mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) grid_beta &lt;- expand.grid(beta1 = seq(-10, 10, 0.5), beta2 = seq(-10, 10, 0.5)) rss_1 &lt;- apply(grid_beta, 1, rss) dat_x &lt;- data.frame(grid_beta, rss_1) ggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + geom_contour() Nótese que los coeficientes ajustados serán diferentes a los del caso no normalizado. Cuando normalizamos antes de ajustar el modelo, las predicciones deben hacerse con entradas normalizadas. La normalización se hace con los mismos valores que se usaron en el entrenamiento (y no recalculando medias y desviaciones estándar con el conjunto de prueba). En cuanto a la forma funcional del predictor f, el problema con entradas normalizadas es equivalente al de las entradas no normalizadas. Asegúrate de esto escribiendo cómo correponden los coeficientes de cada modelo normalizado con los coeficientes del modelo no normalizado. 2.6 Interpretación de modelos lineales Muchas veces se considera que la facilidad de interpretación es una fortaleza del modelo lineal. Esto es en parte cierto, pero hay algunas consideraciones importantes que debemos tomar en cuenta. La interpretación más sólida es la de las predicciones: podemos decir por qué una predicción es alta o baja. Consideremos el ejemplo de cáncer de prostata, por ejemplo: library(tidyr) prostate_completo &lt;- read_csv(file = &#39;datos/prostate.csv&#39;) pr_entrena &lt;- filter(prostate_completo, train) pr_entrena &lt;- pr_entrena %&gt;% mutate(id = 1:nrow(pr_entrena)) #normalizamos pr_entrena_s &lt;- pr_entrena %&gt;% select(id, lcavol, age, lpsa) %&gt;% gather(variable, valor, lcavol:age) %&gt;% group_by(variable) %&gt;% mutate(media = mean(valor), desv = sd(valor)) %&gt;% mutate(valor_s = (valor - media)/desv) pr_modelo &lt;- pr_entrena_s %&gt;% select(id, lpsa, variable, valor_s) %&gt;% spread(variable, valor_s) mod_pr &lt;- lm( lpsa ~ lcavol + age , data = pr_modelo ) round(coefficients(mod_pr), 2) ## (Intercept) lcavol age ## 2.45 0.88 0.02 y observamos el rango de \\(lpsa\\): round(summary(pr_modelo$lpsa), 2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.43 1.67 2.57 2.45 3.37 5.48 Ahora podemos interpretar el predictor: Cuando las variables lcavol y age están en sus media, la predicción de lpsa es 2.5 Si lcavol sube 1 desviación estándar por encima de la media, el predictor de lpsa sube alrededor de 0.9 unidades (de un rango de alrededor de 6 unidades) Si age sube 1 desviación estándar por encima de su media, el predictor de lpsa sube 0.02, lo cual es un movimiento muy chico considerando la variación de lpsa. Así podemos explicar cada predicción - considerando qué variables aportan positiva y cuáles negativamente a la predicción. El camino más seguro es limitarse a hacer este tipo de análisis de las predicciones. Hablamos de entender la estructura predictiva del problema con los datos que tenemos - y no intentamos ir hacia la explicación del fenómeno. Cualquier otra interpretación requiere mucho más cuidados, y requiere una revisión de la especificación correcta del modelo. Parte de estos cuidados se estudian en un curso de regresión desde el punto de vista estadístico, por ejemplo: Variación muestral. Es necesario considerar la variación en nuestras estimaciones de los coeficientes para poder concluir acerca de su relación con el fenómeno (tratable desde punto de vista estadístico, pero hay que checar supuestos). Quizá el error de estimación del coeficiente de lcavol es 2 veces su magnitud - difícilmente podemos concluir algo acerca la relación de lcavol. Efectos no lineales: si la estructura del problema es altamente no lineal, los coeficientes de un modelo lineal no tienen una interpretación clara en relación al fenómeno. Esto también es parcialmente tratable con diagnósticos. set.seed(2112) x &lt;- rnorm(20) y &lt;- x^2 summary(lm(y ~x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7462 -0.5022 -0.3313 0.3435 1.6273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.85344 0.17570 4.857 0.000127 *** ## x 0.04117 0.18890 0.218 0.829929 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7484 on 18 degrees of freedom ## Multiple R-squared: 0.002632, Adjusted R-squared: -0.05278 ## F-statistic: 0.0475 on 1 and 18 DF, p-value: 0.8299 Otros cuidados adicionales se requieren si queremos hacer afirmaciones causales: Variables omitidas: si faltan algunas variables cruciales en el fenómeno que nos interesa, puede ser muy difícil interpretar el resto de los coeficientes en términos del fenómeno Ejemplo: Supongamos que queremos predecir cuánto van a gastar en televisiones samsung ciertas personas que llegan a Amazon. Una variable de entrada es el número de anuncios de televisiones Samsung que recibieron antes de llegar a Amazon. El coeficiente de esta variable es alto (significativo, etc.), así que concluimos que el anuncio causa compras de televisiones Samsung. ¿Qué está mal aquí? El modelo no está mal, sino la interpretación. Cuando las personas están investigando acerca de televisiones, recibe anuncios. La razón es que esta variable nos puede indicar más bien quién está en proceso de compra de una televisión samsung (reciben anuncios) y quién no (no hacen búsquedas relevantes, así que no reciben anuncios). El modelo está mal especificado porque no consideramos que hay otra variable importante, que es el interés de la persona en compra de TVs Samsung. En general, la recomendación es que las interpretaciones causales deben considerarse como preliminares (o sugerencias), y se requiere más análisis y consideraciones antes de poder tener interpretaciones causales sólidas. Ejercicio En el siguiente ejercicio intentamos predecir el porcentaje de grasa corporal (una medición relativamente cara) usando mediciones de varias partes del cuerpo, edad, peso y estatura. Ver script bodyfat_ejercicio.R library(tidyr) dat_grasa &lt;- read_csv(file = &#39;datos/bodyfat.csv&#39;) head(dat_grasa) ## # A tibble: 6 x 14 ## grasacorp edad peso estatura cuello pecho abdomen cadera muslo ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.3 23 154.25 67.75 36.2 93.1 85.2 94.5 59.0 ## 2 6.1 22 173.25 72.25 38.5 93.6 83.0 98.7 58.7 ## 3 25.3 22 154.00 66.25 34.0 95.8 87.9 99.2 59.6 ## 4 10.4 26 184.75 72.25 37.4 101.8 86.4 101.2 60.1 ## 5 28.7 24 184.25 71.25 34.4 97.3 100.0 101.9 63.2 ## 6 20.9 24 210.25 74.75 39.0 104.5 94.4 107.8 66.0 ## # ... with 5 more variables: rodilla &lt;dbl&gt;, tobillo &lt;dbl&gt;, biceps &lt;dbl&gt;, ## # antebrazo &lt;dbl&gt;, muñeca &lt;dbl&gt; nrow(dat_grasa) ## [1] 252 2.7 Solución analítica El problema de mínimos cuadrados tiene una solución de forma cerrada. A partir del gradiente (2.3), podemos igual a cero y resolver (chécalo) para obtener: \\[\\begin{equation*} \\hat{\\beta} = \\left (\\underline{X}\\underline{X}^t \\right)^{-1} \\underline{X}^t\\underline{y} \\end{equation*}\\] Paquetes como lm de R usan como base esta expresión, pero los cálculos se hacen mediante descomposiciones matriciales para más estabilidad (productos de matrices e inversiones). Aunque es posible escalar y/o paralelizar estos cálculos matriciales para problemas grandes, los procedimientos son más delicados. Nuestro enfoque de descenso máximo tiene la ventaja de que es fácil de entender, usar, aplicar a otros problemas con éxito, y además puede escalarse trivialmente, como veremos más adelante (por ejemplo, descenso estocástico). ¡Aunque siempre que se pueda es buena idea usar lm! 2.8 ¿Por qué el modelo lineal funciona bien (muchas veces)? Regresión lineal es un método muy simple, y parecería que debería haber métodos más avanzados que lo superen fácilmente. Para empezar, es poco creíble que el modelo \\[f(X) = b_0 + b_1X_1 + \\cdots b_p X_p\\] se cumple exactamente para el fenómeno que estamos tratando. Pero regresión lineal muchas veces supera a métodos s que intentan construir predictores más complejos. Una de las primeras razones es que podemos ver la aproximación lineal como una aproximación de primer orden a la verdadera \\(f(X)\\), y muchas veces eso es suficiente para producir predicciones razonables. Adicionalmente, otras veces sólo tenemos suficientes datos para hacer una aproximación de primer orden, aún cuando la verdadera \\(f(X)\\) no sea lineal, y resulta que esta aproximación da buenos resultados. Esto es particularmente cierto en problemas de dimensión alta, como veremos a continuación. 2.8.1 k vecinos más cercanos Un método popular, con buen desempeño en varios ejemplos, es el de k-vecinos más cercanos, que consiste en hacer aproximaciones locales directas de \\(f(X)\\). Sea \\({\\mathcal L}\\) un conjunto de entrenamiento. Para \\(k\\) entera fija, y \\(x_0\\) una entrada donde queremos predecir, definimos a \\(N_k(x_0)\\) como el conjunto de los \\(k\\) elementos de \\({\\mathcal L}\\) que tienen \\(x^{(i)}\\) más cercana a \\(x_0\\). Hacemos la predicción \\[\\hat{f}(x_0) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x_0)} y^{(i)}\\] Es decir, promediamos las \\(k\\) \\(y\\)’s con \\(x\\)’s más cercanas a donde queremos predecir. Ejemplo library(ISLR) datos &lt;- Auto[, c(&#39;name&#39;, &#39;weight&#39;,&#39;year&#39;, &#39;mpg&#39;)] datos$peso_kg &lt;- datos$weight*0.45359237 datos$rendimiento_kpl &lt;- datos$mpg*(1.609344/3.78541178) nrow(datos) ## [1] 392 Vamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (2/3 para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba): set.seed(213) datos$muestra_unif &lt;- runif(nrow(datos), 0, 1) datos_entrena &lt;- filter(datos, muestra_unif &gt; 1/3) datos_prueba &lt;- filter(datos, muestra_unif &lt;= 1/3) nrow(datos_entrena) ## [1] 274 nrow(datos_prueba) ## [1] 118 ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point() Consideremos un modelo de \\(k=15\\) vecinos más cercanos. La función de predicción ajustada es entonces: library(kknn) # nótese que no normalizamos entradas - esto también es importante # hacer cuando hacemos vecinos más cercanos, pues en otro caso # las variables con escalas más grandes dominan el cálculo mod_15vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = data_frame(peso_kg=seq(700,2200, by = 10)), k=15) dat_graf &lt;- data_frame(peso_kg = seq(700,2200, by = 10), rendimiento_kpl = predict(mod_15vmc)) ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point(alpha=0.6) + geom_line(data=dat_graf, col=&#39;red&#39;, size = 1.2) Y para \\(k=5\\) vecinos más cercanos: mod_5vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = data_frame(peso_kg=seq(700,2200, by = 10)), k = 5) dat_graf &lt;- data_frame(peso_kg = seq(700,2200, by = 10), rendimiento_kpl = predict(mod_5vmc)) ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point(alpha=0.6) + geom_line(data=dat_graf, col=&#39;red&#39;, size = 1.2) En nuestro caso, los errores de prueba son mod_3vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = datos_prueba, k = 3) mod_15vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = datos_prueba, k = 15) (mean((datos_prueba$rendimiento_kpl-predict(mod_3vmc))^2)) ## [1] 3.346934 (mean((datos_prueba$rendimiento_kpl-predict(mod_15vmc))^2)) ## [1] 2.697658 Pregunta: ¿Cómo escogerías una \\(k\\) adecuada para este problema? Recuerda que adecuada significa que se reduzca a mínimo posible el error de predicción. Como ejercicio, compara los modelos con \\(k = 2, 25, 200\\) utilizando una muestra de prueba. ¿Cuál se desempeña mejor? Da las razones de el mejor o peor desempeño: recuerda que el desempeño en predicción puede sufrir porque la función estimada no es suficiente flexible para capturar patrones importantes, pero también porque parte del ruido se incorpora en la predicción. Por los ejemplos anteriores, vemos que k-vecinos más cercanos puede considerarse como un aproximador universal, que puede adaptarse a cualquier patrón importante que haya en los datos. Entonces, ¿cuál es la razón de utilizar otros métodos como regresión? ¿Por qué el desempeño de regresión sería superior? La maldición de la dimensionalidad El método de k-vecinos más cercanos funciona mejor cuando hay muchas \\(x\\) cercanas a \\(x0\\), de forma que el promedio sea estable (muchas \\(x\\)), y extrapolemos poco (\\(x\\) cercanas). Cuando \\(k\\) es muy chica, nuestras estimaciones son ruidosas, y cuando \\(k\\) es grande y los vecinos están lejos, entonces estamos sesgando la estimación local con datos lejanos a nuestra región de interés. El problema es que en dimensión alta, casi cualquier conjunto de entrenamiento (independientemente del tamaño) sufre fuertemente por uno o ambas dificultades del problema. Ejemplo Consideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, c on una muestra de entrenamiento de 1000 casos. Generamos $x^{i}‘s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso: fun_exp &lt;- function(x) exp(-8*sum(x^2)) x_1 &lt;- runif(1000, -1, 1) x_2 &lt;- runif(1000, -1, 1) dat &lt;- data_frame(x_1 = x_1, x_2 = x_2) dat$y &lt;- apply(dat, 1, fun_exp) ggplot(dat, aes(x = x_1, y = x_2, colour = y)) + geom_point() La mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). Eñ vecino más cercano al origen es dist_origen &lt;- apply(dat, 1, function(x) sqrt(sum(head(x, -1)^2))) mas_cercano_indice &lt;- which.min(dist_origen) mas_cercano &lt;- dat[mas_cercano_indice, ] mas_cercano ## # A tibble: 1 x 3 ## x_1 x_2 y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.03268542 0.01006107 0.9906871 Nuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.9906871, que es bastante cercano al valor verdadero (1). Ahora intentamos hacer lo mismo para dimensión \\(p=8\\). dat_lista &lt;- lapply(1:8, function(i) runif(1000, -1, 1)) dat &lt;- Reduce(cbind, dat_lista) %&gt;% data.frame dat$y &lt;- apply(dat, 1, fun_exp) dist_origen &lt;- apply(dat, 1, function(x) sqrt(sum(head(x, -1)^2))) mas_cercano_indice &lt;- which.min(dist_origen) mas_cercano &lt;- dat[mas_cercano_indice, ] mas_cercano ## init V2 V3 V4 V5 V6 ## 239 0.1612183 0.4117209 0.2546389 -0.226929 0.0774977 0.03897632 ## V7 V8 y ## 239 -0.4959736 0.0382697 0.01073141 Y el resultado es un desastre. Nuestra predicción es mas_cercano$y ## [1] 0.01073141 Necesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (pruébalo). ¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente. En dimensiones altas, todos los conjuntos de entrenamiento factibles se distribuyen de manera rala en el espacio de entradas. Ahora intentamos algo similar con una función que es razonable aproximar con una función lineal: fun_cubica &lt;- function(x) 0.5 * (1 + x[1])^3 set.seed(111) sims_1 &lt;- lapply(1:40, function(i) runif(1000, -1, 1) ) dat &lt;- data.frame(Reduce(cbind, sims_1)) dat$y &lt;- apply(dat, 1, fun_cubica) dist_origen &lt;- apply(dat[, 1:40], 1, function(x) sqrt(sum(x^2))) mas_cercano_indice &lt;- which.min(dist_origen) mas_cercano$y ## [1] 0.01073141 Este no es un resultado muy bueno. Sin embargo, mod_lineal &lt;- lm(y ~ ., data = dat) origen &lt;- data.frame(matrix(rep(0,40), 1, 40)) names(origen) &lt;- names(dat)[1:40] predict(mod_lineal, newdata = origen) ## 1 ## 0.9706047 Donde podemos ver que típicamente la predicción de regresión es mucho mejor que la de 1 vecino más cercano. Esto es porque el modelo explota la estructura aproximadamente lineal del problema. Lo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la infromación para construir predicción con sesgo y varianza bajas. "]
]
