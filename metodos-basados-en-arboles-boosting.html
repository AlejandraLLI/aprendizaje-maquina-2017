<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aprendizaje de máquina</title>
  <meta name="description" content="Notas y material para el curso de aprendizaje de máquina 2017 (ITAM)">
  <meta name="generator" content="bookdown 0.5.4 and GitBook 2.6.7">

  <meta property="og:title" content="Aprendizaje de máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notas y material para el curso de aprendizaje de máquina 2017 (ITAM)" />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-2017" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Aprendizaje de máquina" />
  
  <meta name="twitter:description" content="Notas y material para el curso de aprendizaje de máquina 2017 (ITAM)" />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2017-11-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="metodos-basados-en-arboles.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario y referencias</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#evaluacion"><i class="fa fa-check"></i>Evaluación</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-r-y-rstudio"><i class="fa fa-check"></i>Software: R y Rstudio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias-principales"><i class="fa fa-check"></i>Referencias principales</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#otras-referencias"><i class="fa fa-check"></i>Otras referencias</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado-1"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i><b>1.2.1</b> Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#cuantificacion-de-error-o-precision"><i class="fa fa-check"></i><b>1.4</b> Cuantificación de error o precisión</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.5</b> Tarea de aprendizaje supervisado</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduccion.html"><a href="introduccion.html#observaciones"><i class="fa fa-check"></i><b>1.5.1</b> Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#por-que-tenemos-errores"><i class="fa fa-check"></i><b>1.6</b> ¿Por qué tenemos errores?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#como-estimar-f"><i class="fa fa-check"></i><b>1.7</b> ¿Cómo estimar f?</a></li>
<li class="chapter" data-level="1.8" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.8</b> Resumen</a></li>
<li class="chapter" data-level="1.9" data-path="introduccion.html"><a href="introduccion.html#tarea"><i class="fa fa-check"></i><b>1.9</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion.html"><a href="regresion.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="introduccion.html"><a href="introduccion.html#introduccion"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion.html"><a href="regresion.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion.html"><a href="regresion.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion.html"><a href="regresion.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion.html"><a href="regresion.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion.html"><a href="regresion.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion.html"><a href="regresion.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion.html"><a href="regresion.html#solucion-analitica"><i class="fa fa-check"></i><b>2.7</b> Solución analítica</a></li>
<li class="chapter" data-level="2.8" data-path="regresion.html"><a href="regresion.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.8</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.8.1" data-path="regresion.html"><a href="regresion.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.8.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion.html"><a href="regresion.html#tarea-1"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="logistica.html"><a href="logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="logistica.html"><a href="logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="logistica.html"><a href="logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-10"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="logistica.html"><a href="logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#ejemplo-12"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistica.html"><a href="logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="logistica.html"><a href="logistica.html#ejercicio-1"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="logistica.html"><a href="logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="logistica.html"><a href="logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="logistica.html"><a href="logistica.html#regresion-logistica"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="logistica.html"><a href="logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="logistica.html"><a href="logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="logistica.html"><a href="logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="logistica.html"><a href="logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="logistica.html"><a href="logistica.html#observaciones-adicionales"><i class="fa fa-check"></i><b>3.6</b> Observaciones adicionales</a></li>
<li class="chapter" data-level="3.7" data-path="logistica.html"><a href="logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i><b>3.7</b> Ejercicio: datos de diabetes</a><ul>
<li class="chapter" data-level="" data-path="logistica.html"><a href="logistica.html#tarea-2"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html"><i class="fa fa-check"></i><b>4</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="4.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>4.1</b> Análisis de error para clasificadores binarios</a><ul>
<li class="chapter" data-level="4.1.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#punto-de-corte-para-un-clasificador-binario"><i class="fa fa-check"></i><b>4.1.1</b> Punto de corte para un clasificador binario</a></li>
<li class="chapter" data-level="4.1.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#espacio-roc-de-clasificadores"><i class="fa fa-check"></i><b>4.1.2</b> Espacio ROC de clasificadores</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#perfil-de-un-clasificador-binario-y-curvas-roc"><i class="fa fa-check"></i><b>4.2</b> Perfil de un clasificador binario y curvas ROC</a></li>
<li class="chapter" data-level="4.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>4.3</b> Regresión logística para problemas de más de 2 clases</a><ul>
<li class="chapter" data-level="4.3.1" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>4.3.1</b> Regresión logística multinomial</a></li>
<li class="chapter" data-level="4.3.2" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#interpretacion-de-coeficientes"><i class="fa fa-check"></i><b>4.3.2</b> Interpretación de coeficientes</a></li>
<li class="chapter" data-level="4.3.3" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#ejemplo-clasificacion-de-digitos-con-regresion-multinomial"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo: Clasificación de dígitos con regresión multinomial</a></li>
<li class="chapter" data-level="" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#discusion"><i class="fa fa-check"></i>Discusión</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="mas-sobre-problemas-de-clasificacion.html"><a href="mas-sobre-problemas-de-clasificacion.html#descenso-en-gradiente-para-regresion-multinomial-logistica"><i class="fa fa-check"></i><b>4.4</b> Descenso en gradiente para regresión multinomial logística</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regularizacion.html"><a href="regularizacion.html"><i class="fa fa-check"></i><b>5</b> Regularización</a><ul>
<li class="chapter" data-level="5.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-de-predictores"><i class="fa fa-check"></i><b>5.1</b> Sesgo y varianza de predictores</a><ul>
<li class="chapter" data-level="5.1.1" data-path="regularizacion.html"><a href="regularizacion.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>5.1.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="5.1.2" data-path="regularizacion.html"><a href="regularizacion.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>5.1.2</b> Reduciendo varianza de los coeficientes</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-ridge"><i class="fa fa-check"></i><b>5.2</b> Regularización ridge</a><ul>
<li class="chapter" data-level="5.2.1" data-path="regularizacion.html"><a href="regularizacion.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>5.2.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regularizacion.html"><a href="regularizacion.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>5.3</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="5.3.1" data-path="regularizacion.html"><a href="regularizacion.html#validacion-cruzada"><i class="fa fa-check"></i><b>5.3.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="" data-path="regularizacion.html"><a href="regularizacion.html#ejercicio-5"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="regularizacion.html"><a href="regularizacion.html#regularizacion-lasso"><i class="fa fa-check"></i><b>5.4</b> Regularización lasso</a></li>
<li class="chapter" data-level="5.5" data-path="regularizacion.html"><a href="regularizacion.html#tarea-3"><i class="fa fa-check"></i><b>5.5</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html"><i class="fa fa-check"></i><b>6</b> Extensiones para regresión lineal y logística</a><ul>
<li class="chapter" data-level="6.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#como-hacer-mas-flexible-el-modelo-lineal"><i class="fa fa-check"></i><b>6.1</b> Cómo hacer más flexible el modelo lineal</a></li>
<li class="chapter" data-level="6.2" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#transformacion-de-entradas"><i class="fa fa-check"></i><b>6.2</b> Transformación de entradas</a></li>
<li class="chapter" data-level="6.3" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#variables-cualitativas"><i class="fa fa-check"></i><b>6.3</b> Variables cualitativas</a></li>
<li class="chapter" data-level="6.4" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#interacciones"><i class="fa fa-check"></i><b>6.4</b> Interacciones</a></li>
<li class="chapter" data-level="6.5" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#categorizacion-de-variables"><i class="fa fa-check"></i><b>6.5</b> Categorización de variables</a></li>
<li class="chapter" data-level="6.6" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#splines"><i class="fa fa-check"></i><b>6.6</b> Splines</a><ul>
<li class="chapter" data-level="6.6.1" data-path="extensiones-para-regresion-lineal-y-logistica.html"><a href="extensiones-para-regresion-lineal-y-logistica.html#cuando-usar-estas-tecnicas"><i class="fa fa-check"></i><b>6.6.1</b> ¿Cuándo usar estas técnicas?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html"><i class="fa fa-check"></i><b>7</b> Redes neuronales (parte 1)</a><ul>
<li class="chapter" data-level="7.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#introduccion-a-redes-neuronales"><i class="fa fa-check"></i><b>7.1</b> Introducción a redes neuronales</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-construyen-entradas-las-redes-neuronales"><i class="fa fa-check"></i>¿Cómo construyen entradas las redes neuronales?</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#como-ajustar-los-parametros"><i class="fa fa-check"></i>¿Cómo ajustar los parámetros?</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#interacciones-en-redes-neuronales"><i class="fa fa-check"></i><b>7.2</b> Interacciones en redes neuronales</a></li>
<li class="chapter" data-level="7.3" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-en-redes-feed-forward."><i class="fa fa-check"></i><b>7.3</b> Cálculo en redes: feed-forward.</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#notacion-1"><i class="fa fa-check"></i>Notación</a></li>
<li class="chapter" data-level="7.4" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#feed-forward"><i class="fa fa-check"></i><b>7.4</b> Feed forward</a></li>
<li class="chapter" data-level="7.5" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#backpropagation-calculo-del-gradiente"><i class="fa fa-check"></i><b>7.5</b> Backpropagation: cálculo del gradiente</a><ul>
<li class="chapter" data-level="7.5.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#calculo-para-un-caso-de-entrenamiento"><i class="fa fa-check"></i><b>7.5.1</b> Cálculo para un caso de entrenamiento</a></li>
<li class="chapter" data-level="7.5.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#algoritmo-de-backpropagation"><i class="fa fa-check"></i><b>7.5.2</b> Algoritmo de backpropagation</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ajuste-de-parametros-introduccion"><i class="fa fa-check"></i><b>7.6</b> Ajuste de parámetros (introducción)</a><ul>
<li class="chapter" data-level="7.6.1" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#ejemplo-31"><i class="fa fa-check"></i><b>7.6.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.6.2" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#hiperparametros-busqueda-manual"><i class="fa fa-check"></i><b>7.6.2</b> Hiperparámetros: búsqueda manual</a></li>
<li class="chapter" data-level="7.6.3" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#hiperparametros-busqueda-en-grid"><i class="fa fa-check"></i><b>7.6.3</b> Hiperparámetros: búsqueda en grid</a></li>
<li class="chapter" data-level="7.6.4" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#hiperparametros-busqueda-aleatoria"><i class="fa fa-check"></i><b>7.6.4</b> Hiperparámetros: búsqueda aleatoria</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#tarea-para-25-de-septiembre"><i class="fa fa-check"></i>Tarea (para 25 de septiembre)</a></li>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-1.html"><a href="redes-neuronales-parte-1.html#tarea-2-de-octubre"><i class="fa fa-check"></i>Tarea (2 de octubre)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales (parte 2)</a><ul>
<li class="chapter" data-level="8.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#descenso-estocastico"><i class="fa fa-check"></i><b>8.1</b> Descenso estocástico</a></li>
<li class="chapter" data-level="8.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>8.2</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="8.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>8.3</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="8.4" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.4</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>8.5</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="8.5.1" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>8.5.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="8.5.2" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#momento"><i class="fa fa-check"></i><b>8.5.2</b> Momento</a></li>
<li class="chapter" data-level="8.5.3" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#otras-variaciones"><i class="fa fa-check"></i><b>8.5.3</b> Otras variaciones</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ajuste-de-redes-con-descenso-estocastico"><i class="fa fa-check"></i><b>8.6</b> Ajuste de redes con descenso estocástico</a></li>
<li class="chapter" data-level="8.7" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#activaciones-relu"><i class="fa fa-check"></i><b>8.7</b> Activaciones relu</a></li>
<li class="chapter" data-level="8.8" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#dropout-para-regularizacion"><i class="fa fa-check"></i><b>8.8</b> Dropout para regularización</a><ul>
<li class="chapter" data-level="" data-path="redes-neuronales-parte-2.html"><a href="redes-neuronales-parte-2.html#ejemplo-35"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html"><i class="fa fa-check"></i><b>9</b> Redes convolucionales</a><ul>
<li class="chapter" data-level="9.1" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales"><i class="fa fa-check"></i><b>9.1</b> Filtros convolucionales</a><ul>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-en-una-dimension"><i class="fa fa-check"></i>Filtros en una dimensión</a></li>
<li class="chapter" data-level="" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-en-dos-dimensiones"><i class="fa fa-check"></i>Filtros convolucionales en dos dimensiones</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#filtros-convolucionales-para-redes-neuronales"><i class="fa fa-check"></i><b>9.2</b> Filtros convolucionales para redes neuronales</a></li>
<li class="chapter" data-level="9.3" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#capas-de-agregacion-pooling"><i class="fa fa-check"></i><b>9.3</b> Capas de agregación (pooling)</a></li>
<li class="chapter" data-level="9.4" data-path="redes-convolucionales.html"><a href="redes-convolucionales.html#ejemplo-arquitectura-lenet"><i class="fa fa-check"></i><b>9.4</b> Ejemplo (arquitectura LeNet):</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>10</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="10.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>10.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="10.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>10.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="10.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>10.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="10.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>10.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="10.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>10.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="10.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>10.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="10.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>10.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="10.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>10.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>11</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="11.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>11.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="11.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>11.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="11.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>11.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="11.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>11.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="11.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>11.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="11.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>11.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="11.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>11.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="11.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>11.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="11.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>11.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="11.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>11.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="11.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>11.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>11.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="11.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-42"><i class="fa fa-check"></i><b>11.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="11.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>11.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>11.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="11.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>11.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="11.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-43"><i class="fa fa-check"></i><b>11.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="11.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>11.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="11.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>11.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="11.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>11.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="11.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles-aleatorios"><i class="fa fa-check"></i><b>11.3.6</b> Ventajas y desventajas de árboles aleatorios</a></li>
<li class="chapter" data-level="11.3.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tarea-para-23-de-octubre"><i class="fa fa-check"></i><b>11.3.7</b> Tarea (para 23 de octubre)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html"><i class="fa fa-check"></i><b>12</b> Métodos basados en árboles: boosting</a><ul>
<li class="chapter" data-level="12.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#forward-stagewise-additive-modeling-fsam"><i class="fa fa-check"></i><b>12.1</b> Forward stagewise additive modeling (FSAM)</a></li>
<li class="chapter" data-level="12.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-1"><i class="fa fa-check"></i><b>12.2</b> Discusión</a></li>
<li class="chapter" data-level="12.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-fsam"><i class="fa fa-check"></i><b>12.3</b> Algoritmo FSAM</a></li>
<li class="chapter" data-level="12.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#fsam-para-clasificacion-binaria."><i class="fa fa-check"></i><b>12.4</b> FSAM para clasificación binaria.</a></li>
<li class="chapter" data-level="12.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>12.5</b> Gradient boosting</a></li>
<li class="chapter" data-level="12.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#algoritmo-de-gradient-boosting"><i class="fa fa-check"></i><b>12.6</b> Algoritmo de gradient boosting</a></li>
<li class="chapter" data-level="12.7" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#funciones-de-perdida"><i class="fa fa-check"></i><b>12.7</b> Funciones de pérdida</a><ul>
<li class="chapter" data-level="12.7.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-adaboost-opcional"><i class="fa fa-check"></i><b>12.7.1</b> Discusión: adaboost (opcional)</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-46"><i class="fa fa-check"></i>Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#modificaciones-de-gradient-boosting"><i class="fa fa-check"></i><b>12.8</b> Modificaciones de Gradient Boosting</a><ul>
<li class="chapter" data-level="12.8.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tasa-de-aprendizaje-shrinkage"><i class="fa fa-check"></i><b>12.8.1</b> Tasa de aprendizaje (shrinkage)</a></li>
<li class="chapter" data-level="12.8.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#submuestreo-bag.fraction"><i class="fa fa-check"></i><b>12.8.2</b> Submuestreo (bag.fraction)</a></li>
<li class="chapter" data-level="12.8.3" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#numero-de-arboles-m"><i class="fa fa-check"></i><b>12.8.3</b> Número de árboles M</a></li>
<li class="chapter" data-level="12.8.4" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tamano-de-arboles"><i class="fa fa-check"></i><b>12.8.4</b> Tamaño de árboles</a></li>
<li class="chapter" data-level="12.8.5" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#controlar-numero-de-casos-para-cortes"><i class="fa fa-check"></i><b>12.8.5</b> Controlar número de casos para cortes</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-47"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="12.8.6" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#evaluacion-con-validacion-cruzada."><i class="fa fa-check"></i><b>12.8.6</b> Evaluación con validación cruzada.</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#graficas-de-dependencia-parcial"><i class="fa fa-check"></i><b>12.9</b> Gráficas de dependencia parcial</a><ul>
<li class="chapter" data-level="12.9.1" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#dependencia-parcial"><i class="fa fa-check"></i><b>12.9.1</b> Dependencia parcial</a></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#ejemplo-48"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="12.9.2" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#discusion-2"><i class="fa fa-check"></i><b>12.9.2</b> Discusión</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="metodos-basados-en-arboles-boosting.html"><a href="metodos-basados-en-arboles-boosting.html#tarea-5"><i class="fa fa-check"></i>Tarea</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje de máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="metodos-basados-en-arboles-boosting" class="section level1">
<h1><span class="header-section-number">Clase 12</span> Métodos basados en árboles: boosting</h1>
<p>Boosting también utiliza la idea de un “ensamble” de árboles. La diferencia grande con bagging y bosques aleatorios en que la sucesión de árboles de boosting se ‘adapta’ al comportamiento del predictor a lo largo de las iteraciones, haciendo reponderaciones de los datos de entrenamiento para que el algoritmo se concentre en las predicciones más pobres. Boosting generalmente funciona bien con árboles chicos (cada uno con sesgo alto), mientras que bosques aleatorios funciona con árboles grandes (sesgo bajo).</p>
<ul>
<li><p>En boosting usamos muchos árboles chicos adaptados secuencialmente. La disminución del sesgo proviene de usar distintos árboles que se encargan de adaptar el predictor a distintas partes del conjunto de entrenamiento. El control de varianza se logra con tasas de aprendizaje y tamaño de árboles, como veremos más adelante.</p></li>
<li><p>En bosques aleatorios usamos muchos árboles grandes, cada uno con una muestra de entrenamiento perturbada (bootstrap). El control de varianza se logra promediando sobre esas muestras bootstrap de entrenamiento.</p></li>
</ul>
<p>Igual que bosques aleatorios, boosting es también un método que generalmente tiene alto poder predictivo.</p>
<div id="forward-stagewise-additive-modeling-fsam" class="section level2">
<h2><span class="header-section-number">12.1</span> Forward stagewise additive modeling (FSAM)</h2>
<p>Aunque existen versiones de boosting (Adaboost) desde los 90s, una buena manera de entender los algoritmos es mediante un proceso general de modelado por estapas (FSAM).</p>
</div>
<div id="discusion-1" class="section level2">
<h2><span class="header-section-number">12.2</span> Discusión</h2>
<p>Consideramos primero un problema de <em>regresión</em>, que queremos atacar con un predictor de la forma <span class="math display">\[f(x) = \sum_{k=1}^m \beta_k b_k(x),\]</span> donde los <span class="math inline">\(b_k\)</span> son árboles. Podemos absorber el coeficiente <span class="math inline">\(\beta_k\)</span> dentro del árbol <span class="math inline">\(b_k(x)\)</span>, y escribimos</p>
<p><span class="math display">\[f(x) = \sum_{k=1}^m T_k(x),\]</span></p>
<p>Para ajustar este tipo de modelos, buscamos minimizar la pérdida de entrenamiento:</p>
<span class="math display">\[\begin{equation}
\min \sum_{i=1}^N L(y^{(i)}, \sum_{k=1}^M T_k(x^{(i)}))
\end{equation}\]</span>
<p>Este puede ser un problema difícil, dependiendo de la familia que usemos para los árboles <span class="math inline">\(T_k\)</span>, y sería difícil resolver por fuerza bruta. Para resolver este problema, podemos intentar una heurística secuencial o por etapas:</p>
<p>Si tenemos <span class="math display">\[f_{m-1}(x) = \sum_{k=1}^{m-1} T_k(x),\]</span></p>
<p>intentamos resolver el problema (añadir un término adicional)</p>
<span class="math display">\[\begin{equation}
\min_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))
\end{equation}\]</span>
<p>Por ejemplo, para pérdida cuadrática (en regresión), buscamos resolver</p>
<span class="math display">\[\begin{equation}
\min_{T} \sum_{i=1}^N (y^{(i)} - f_{m-1}(x^{(i)}) - T(x^{(i)}))^2
\end{equation}\]</span>
Si ponemos <span class="math display">\[ r_{m-1}^{(i)} = y^{(i)} - f_{m-1}(x^{(i)}),\]</span> que es el error para el caso <span class="math inline">\(i\)</span> bajo el modelo <span class="math inline">\(f_{m-1}\)</span>, entonces reescribimos el problema anterior como
<span class="math display">\[\begin{equation}
\min_{T} \sum_{i=1}^N ( r_{m-1}^{(i)} - T(x^{(i)}))^2
\end{equation}\]</span>
<p>Este problema consiste en <em>ajustar un árbol a los residuales o errores del paso anterior</em>. Otra manera de decir esto es que añadimos un término adicional que intenta corregir los que el modelo anterior no pudo predecir bien. La idea es repetir este proceso para ir reduciendo los residuales, agregando un árbol a la vez.</p>

<div class="comentario">
La primera idea central de boosting es concentrarnos, en el siguiente paso, en los datos donde tengamos errores, e intentar corregir añadiendo un término adicional al modelo.
</div>

</div>
<div id="algoritmo-fsam" class="section level2">
<h2><span class="header-section-number">12.3</span> Algoritmo FSAM</h2>
<p>Esta idea es la base del siguiente algoritmo:</p>

<div class="comentario">
<p><strong>Algoritmo FSAM</strong> (forward stagewise additive modeling)</p>
<ol style="list-style-type: decimal">
<li>Tomamos <span class="math inline">\(f_0(x)=0\)</span></li>
<li>Para <span class="math inline">\(m=1\)</span> hasta <span class="math inline">\(M\)</span>,</li>
</ol>
<ul>
<li>Resolvemos <span class="math display">\[T_m = argmin_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))\]</span></li>
<li>Ponemos <span class="math display">\[f_m(x) = f_{m-1}(x) + T_m(x)\]</span></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Nuestro predictor final es <span class="math inline">\(f(x) = \sum_{m=1}^M T_(x)\)</span>.
</div>
</li>
</ol>
<p><strong>Observaciones</strong>: Generalmente los árboles sobre los que optimizamos están restringidos a una familia relativamente chica: por ejemplo, árboles de profundidad no mayor a <span class="math inline">\(2,3,\ldots, 8\)</span>.</p>
<p>Este algoritmo se puede aplicar directamente para problemas de regresión, como vimos en la discusión anterior: simplemente hay que ajustar árboles a los residuales del modelo del paso anterior. Sin embargo, no está claro cómo aplicarlo cuando la función de pérdida no es mínimos cuadrados (por ejemplo, regresión logística).</p>
<div id="ejemplo-regresion" class="section level4 unnumbered">
<h4>Ejemplo (regresión)</h4>
<p>Podemos hacer FSAM directamente sobre un problema de regresión.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">227818</span>)
<span class="kw">library</span>(rpart)
<span class="kw">library</span>(tidyverse)
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="dv">30</span>)
y &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">ifelse</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">0</span>, <span class="kw">sqrt</span>(x)) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>, <span class="dv">0</span>, <span class="fl">0.5</span>)
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)</code></pre></div>
<p>Pondremos los árboles de cada paso en una lista. Podemos comenzar con una constante en lugar de 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">list</span>()
arboles_fsam[[<span class="dv">1</span>]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(y<span class="op">~</span>x, <span class="dt">data =</span> dat, 
                           <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxdepth=</span><span class="dv">0</span>))
arboles_fsam[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## n= 200 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 200 5370.398 4.675925 *</code></pre>
<p>Ahora construirmos nuestra función de predicción y el paso que agrega un árbol</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predecir_arboles &lt;-<span class="st"> </span><span class="cf">function</span>(arboles_fsam, x){
  preds &lt;-<span class="st"> </span><span class="kw">lapply</span>(arboles_fsam, <span class="cf">function</span>(arbol){
    <span class="kw">predict</span>(arbol, <span class="kw">data.frame</span>(<span class="dt">x=</span>x))
  })
  <span class="kw">reduce</span>(preds, <span class="st">`</span><span class="dt">+</span><span class="st">`</span>)
}
agregar_arbol &lt;-<span class="st"> </span><span class="cf">function</span>(arboles_fsam, dat, <span class="dt">plot=</span><span class="ot">TRUE</span>){
  n &lt;-<span class="st"> </span><span class="kw">length</span>(arboles_fsam)
  preds &lt;-<span class="st"> </span><span class="kw">predecir_arboles</span>(arboles_fsam, <span class="dt">x=</span>dat<span class="op">$</span>x)
  dat<span class="op">$</span>res &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>preds
  arboles_fsam[[n<span class="op">+</span><span class="dv">1</span>]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(res <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> dat, 
                           <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxdepth =</span> <span class="dv">1</span>))
  dat<span class="op">$</span>preds_nuevo &lt;-<span class="st"> </span><span class="kw">predict</span>(arboles_fsam[[n<span class="op">+</span><span class="dv">1</span>]])
  dat<span class="op">$</span>preds &lt;-<span class="st"> </span><span class="kw">predecir_arboles</span>(arboles_fsam, <span class="dt">x=</span>dat<span class="op">$</span>x)
  g_res &lt;-<span class="st"> </span><span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>preds_nuevo)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>res)) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&#39;Residuales&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>))
  g_agregado &lt;-<span class="st"> </span><span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x=</span>x)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>preds), <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>,
                                                  <span class="dt">size=</span><span class="fl">1.1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>y)) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">title =</span><span class="st">&#39;Ajuste&#39;</span>)
  <span class="cf">if</span>(plot){
    <span class="kw">print</span>(g_res)
    <span class="kw">print</span>(g_agregado)
  }
  arboles_fsam
}</code></pre></div>
<p>Ahora construiremos el primer árbol. Usaremos ‘troncos’ (stumps), árboles con un solo corte: Los primeros residuales son simplemente las <span class="math inline">\(y\)</span>’s observadas</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre></div>
<pre><code>## Warning: Removed 8 rows containing missing values (geom_point).</code></pre>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-7-1.png" width="384" /><img src="12-arboles-2_files/figure-html/unnamed-chunk-7-2.png" width="384" /></p>
<p>Ajustamos un árbol de regresión a los residuales:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-8-1.png" width="384" /><img src="12-arboles-2_files/figure-html/unnamed-chunk-8-2.png" width="384" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-9-1.png" width="384" /><img src="12-arboles-2_files/figure-html/unnamed-chunk-9-2.png" width="384" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-10-1.png" width="384" /><img src="12-arboles-2_files/figure-html/unnamed-chunk-10-2.png" width="384" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-11-1.png" width="384" /><img src="12-arboles-2_files/figure-html/unnamed-chunk-11-2.png" width="384" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-12-1.png" width="384" /><img src="12-arboles-2_files/figure-html/unnamed-chunk-12-2.png" width="384" /></p>
<p>Después de 20 iteraciones obtenemos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">19</span>){
arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat, <span class="dt">plot =</span> <span class="ot">FALSE</span>)
}
arboles_fsam &lt;-<span class="st"> </span><span class="kw">agregar_arbol</span>(arboles_fsam, dat)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-13-1.png" width="384" /><img src="12-arboles-2_files/figure-html/unnamed-chunk-13-2.png" width="384" /></p>
</div>
</div>
<div id="fsam-para-clasificacion-binaria." class="section level2">
<h2><span class="header-section-number">12.4</span> FSAM para clasificación binaria.</h2>
<p>Para problemas de clasificación, no tiene mucho sentido trabajar con un modelo aditivo sobre las probabilidades:</p>
<p><span class="math display">\[p(x) = \sum_{k=1}^m T_k(x),\]</span></p>
<p>Así que hacemos lo mismo que en regresión logística. Ponemos</p>
<p><span class="math display">\[f(x) = \sum_{k=1}^m T_k(x),\]</span></p>
<p>y entonces las probabilidades son <span class="math display">\[p(x) = h(f(x)),\]</span></p>
<p>donde <span class="math inline">\(h(z)=1/(1+e^{-z})\)</span> es la función logística. La optimización de la etapa <span class="math inline">\(m\)</span> según fsam es</p>
<span class="math display" id="eq:fsam-paso">\[\begin{equation}
T = argmin_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))
\tag{12.1}
\end{equation}\]</span>
<p>y queremos usar la devianza como función de pérdida. Por razones de comparación (con nuestro libro de texto y con el algoritmo Adaboost que mencionaremos más adelante), escogemos usar <span class="math display">\[y \in \{1,-1\}\]</span></p>
<p>en lugar de nuestro tradicional <span class="math inline">\(y \in \{1,0\}\)</span>. En ese caso, la devianza binomial se ve como</p>
<p><span class="math display">\[L(y, z) = -\left [ (y+1)\log h(z) - (y-1)\log(1-h(z))\right ],\]</span> que a su vez se puede escribir como (demostrar):</p>
<p><span class="math display">\[L(y,z) = 2\log(1+e^{-yz})\]</span> Ahora consideremos cómo se ve nuestro problema de optimización:</p>
<p><span class="math display">\[T = argmin_{T} 2\sum_{i=1}^N \log (1+ e^{-y^{(i)}(f_{m-1}(x^{(i)}) + T(x^{(i)})})\]</span></p>
<p>Nótese que sólo optimizamos con respecto a <span class="math inline">\(T\)</span>, así que podemos escribir</p>
<p><span class="math display">\[T = argmin_{T} 2\sum_{i=1}^N \log (1+ d_{m,i}e^{- y^{(i)}T(x^{(i)})})\]</span></p>
<p>Y vemos que el problema es más difícil que en regresión. No podemos usar un ajuste de árbol usual de regresión o clasificación, <em>como hicimos en regresión</em>. No está claro, por ejemplo, cuál debería ser el residual que tenemos que ajustar (aunque parece un problema donde los casos de entrenamiento están ponderados por <span class="math inline">\(d_{m,i}\)</span>). Una solución para resolver aproximadamente este problema de minimización, es <strong>gradient boosting</strong>.</p>
</div>
<div id="gradient-boosting" class="section level2">
<h2><span class="header-section-number">12.5</span> Gradient boosting</h2>
<p>La idea de gradient boosting es replicar la idea del residual en regresión, y usar árboles de regresión para resolver <a href="metodos-basados-en-arboles-boosting.html#eq:fsam-paso">(12.1)</a>.</p>
<p>Gradient boosting es una técnica general para funciones de pérdida generales.Regresamos entonces a nuestro problema original</p>
<p><span class="math display">\[(\beta_m, b_m) = argmin_{T} \sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))\]</span></p>
<p>La pregunta es: ¿hacia dónde tenemos qué mover la predicción de <span class="math inline">\(f_{m-1}(x^{(i)})\)</span> sumando el término <span class="math inline">\(T(x^{(i)})\)</span>? Consideremos un solo término de esta suma, y denotemos <span class="math inline">\(z_i = T(x^{(i)})\)</span>. Queremos agregar una cantidad <span class="math inline">\(z_i\)</span> tal que el valor de la pérdida <span class="math display">\[L(y, f_{m-1}(x^{(i)})+z_i)\]</span> se reduzca. Entonces sabemos que podemos mover la z en la dirección opuesta al gradiente</p>
<p><span class="math display">\[z_i = -\gamma \frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span></p>
Sin embargo, necesitamos que las <span class="math inline">\(z_i\)</span> estén generadas por una función <span class="math inline">\(T(x)\)</span> que se pueda evaluar en toda <span class="math inline">\(x\)</span>. Quisiéramos que <span class="math display">\[T(x^{(i)})\approx -\gamma \frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span> Para tener esta aproximación, podemos poner <span class="math display">\[g_{i,m} = -\frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span> e intentar resolver
<span class="math display" id="eq:min-cuad-boost">\[\begin{equation}
\min_T \sum_{i=1}^n (g_{i,m} - T(x^{(i)}))^2,
\tag{12.2}
\end{equation}\]</span>
<p>es decir, intentamos replicar los gradientes lo más que sea posible. <strong>Este problema lo podemos resolver con un árbol usual de regresión</strong>. Finalmente, podríamos escoger <span class="math inline">\(\nu\)</span> (tamaño de paso) suficientemente chica y ponemos <span class="math display">\[f_m(x) = f_{m-1}(x)+\nu T(x).\]</span></p>
<p>Podemos hacer un refinamiento adicional que consiste en encontrar los cortes del árbol <span class="math inline">\(T\)</span> según <a href="metodos-basados-en-arboles-boosting.html#eq:min-cuad-boost">(12.2)</a>, pero optimizando por separado los valores que T(x) toma en cada una de las regiones encontradas.</p>
</div>
<div id="algoritmo-de-gradient-boosting" class="section level2">
<h2><span class="header-section-number">12.6</span> Algoritmo de gradient boosting</h2>

<div class="comentario">
<p><strong>Gradient boosting</strong> (versión simple)</p>
<ol style="list-style-type: decimal">
<li><p>Inicializar con <span class="math inline">\(f_0(x) =\gamma\)</span></p></li>
<li><p>Para <span class="math inline">\(m=0,1,\ldots, M\)</span>,</p></li>
</ol>
<ul>
<li><p>Para <span class="math inline">\(i=1,\ldots, n\)</span>, calculamos el residual <span class="math display">\[r_{i,m}=-\frac{\partial L}{\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\]</span></p></li>
<li><p>Ajustamos un árbol de regresión a la respuesta <span class="math inline">\(r_{1,m},r_{2,m},\ldots, r_{n,m}\)</span>. Supongamos que tiene regiones <span class="math inline">\(R_{j,m}\)</span>.</p></li>
<li>Resolvemos (optimizamos directamente el valor que toma el árbol en cada región - este es un problema univariado, más fácil de resolver) <span class="math display">\[\gamma_{j,m} = argmin_\gamma \sum_{x^{(i)}\in R_{j,m}} L(y^{(i)},f_{m-1}(x^{i})+\gamma )\]</span> para cada región <span class="math inline">\(R_{j,m}\)</span> del árbol del inciso anterior.</li>
<li>Actualizamos <span class="math display">\[f_m (x) = f_{m-1}(x) + \sum_j \gamma_{j,m} I(x\in R_{j,m})\]</span></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>El predictor final es <span class="math inline">\(f_M(x)\)</span>.
</div>
</li>
</ol>
</div>
<div id="funciones-de-perdida" class="section level2">
<h2><span class="header-section-number">12.7</span> Funciones de pérdida</h2>
<p>Para aplicar gradient boosting, tenemos primero que poder calcular el gradiente de la función de pérdida. Algunos ejemplos populares son:</p>
<ul>
<li>Pérdida cuadrática: <span class="math inline">\(L(y,f(x))=(y-f(x))^2\)</span>, <span class="math inline">\(\frac{\partial L}{\partial z} = -2(y-f(x))\)</span>.</li>
<li>Pérdida absoluta (más robusta a atípicos que la cuadrática) <span class="math inline">\(L(y,f(x))=|y-f(x)|\)</span>, <span class="math inline">\(\frac{\partial L}{\partial z} = signo(y-f(x))\)</span>.</li>
<li>Devianza binomial <span class="math inline">\(L(y, f(x))\)</span> = -(1+e^{-yf(x)}), <span class="math inline">\(y\in\{-1,1\}\)</span>, <span class="math inline">\(\frac{\partial L}{\partial z} = I(y=1) - h(f(x))\)</span>.</li>
<li>Adaboost, pérdida exponencial (para clasificación) <span class="math inline">\(L(y,z) = e^{-yf(x)}\)</span>, <span class="math inline">\(y\in\{-1,1\}\)</span>, <span class="math inline">\(\frac{\partial L}{\partial z} = -ye^{-yf(x)}\)</span>.</li>
</ul>
<div id="discusion-adaboost-opcional" class="section level3">
<h3><span class="header-section-number">12.7.1</span> Discusión: adaboost (opcional)</h3>
<p>Adaboost es uno de los algoritmos originales para boosting, y no es necesario usar gradient boosting para aplicarlo. La razón es que los árboles de clasificación <span class="math inline">\(T(x)\)</span> toman valores <span class="math inline">\(T(x)\in \{-1,1\}\)</span>, y el paso de optimización <a href="metodos-basados-en-arboles-boosting.html#eq:fsam-paso">(12.1)</a> de cada árbol queda</p>
<p><span class="math display">\[T = argmin_{T} \sum_{i=1}^N e^{-y^{(i)}f_{m-1}(x^{(i)})} e^{-y^{(i)}T(x^{(i)})}
\]</span> <span class="math display">\[T = argmin_{T} \sum_{i=1}^N d_{m,i} e^{-y^{(i)}T(x^{(i)})}
\]</span> De modo que la función objetivo toma dos valores: Si <span class="math inline">\(T(x^{i})\)</span> clasifica correctamente, entonces <span class="math inline">\(e^{-y^{(i)}T(x^{(i)})}=e^{-1}\)</span>, y si clasifica incorrectamente <span class="math inline">\(e^{-y^{(i)}T(x^{(i)})}=e^{1}\)</span>. Podemos entonces encontrar el árbol <span class="math inline">\(T\)</span> construyendo un árbol usual pero con datos ponderados por <span class="math inline">\(d_{m,i}\)</span>, donde buscamos maximizar la tasa de clasificación correcta (puedes ver más en nuestro libro de texto, o en <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-ESL">2017</a>)</span>.</p>
<p>¿Cuáles son las consecuencias de usar la pérdida exponencial? Una es que perdemos la conexión con los modelos logísticos e interpretación de probabilidad que tenemos cuando usamos la devianza. Sin embargo, son similares: compara cómo se ve la devianza (como la formulamos arriba, con <span class="math inline">\(y\in\{-1,1\}\)</span>) con la pérdida exponencial.</p>
</div>
<div id="ejemplo-46" class="section level3 unnumbered">
<h3>Ejemplo</h3>
<p>Podemos usar el paquete de R <em>gbm</em> para hacer gradient boosting. Para el caso de precios de casas de la sección anterior (un problema de regresión).</p>
<p>Fijaremos el número de árboles en 200, de profundidad 3, usando 75% de la muestra para entrenar y el restante para validación:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gbm)
entrena &lt;-<span class="st"> </span><span class="kw">read_rds</span>(<span class="st">&#39;datos/ameshousing-entrena-procesado.rds&#39;</span>)
<span class="kw">set.seed</span>(<span class="dv">23411</span>)

ajustar_boost &lt;-<span class="st"> </span><span class="cf">function</span>(entrena, ...){
  mod_boosting &lt;-<span class="st"> </span><span class="kw">gbm</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span>.,  <span class="dt">data =</span> entrena,
                <span class="dt">distribution =</span> <span class="st">&#39;gaussian&#39;</span>,
                <span class="dt">n.trees =</span> <span class="dv">200</span>, 
                <span class="dt">interaction.depth =</span> <span class="dv">3</span>,
                <span class="dt">shrinkage =</span> <span class="dv">1</span>, <span class="co"># tasa de aprendizaje</span>
                <span class="dt">bag.fraction =</span> <span class="dv">1</span>,
                <span class="dt">train.fraction =</span> <span class="fl">0.75</span>)
  mod_boosting
}

house_boosting &lt;-<span class="st"> </span><span class="kw">ajustar_boost</span>(entrena)
dat_entrenamiento &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">entrena =</span> <span class="kw">sqrt</span>(house_boosting<span class="op">$</span>train.error),
                                <span class="dt">valida =</span> <span class="kw">sqrt</span>(house_boosting<span class="op">$</span>valid.error),
                                <span class="dt">n_arbol =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(house_boosting<span class="op">$</span>train.error)) <span class="op">%&gt;%</span>
<span class="st">                      </span><span class="kw">gather</span>(tipo, valor, <span class="op">-</span>n_arbol)
<span class="kw">print</span>(house_boosting)</code></pre></div>
<pre><code>## gbm(formula = log(vSalePrice) ~ ., distribution = &quot;gaussian&quot;, 
##     data = entrena, n.trees = 200, interaction.depth = 3, shrinkage = 1, 
##     bag.fraction = 1, train.fraction = 0.75)
## A gradient boosted model with gaussian loss function.
## 200 iterations were performed.
## The best test-set iteration was 20.
## There were 79 predictors of which 28 had non-zero influence.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_entrenamiento, <span class="kw">aes</span>(<span class="dt">x=</span>n_arbol, <span class="dt">y=</span>valor, <span class="dt">colour=</span>tipo, <span class="dt">group=</span>tipo)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-15-1.png" width="480" /></p>
<p>Que se puede graficar también así:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">gbm.perf</span>(house_boosting)</code></pre></div>
<pre><code>## Using test method...</code></pre>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-16-1.png" width="480" /></p>
<pre><code>## [1] 20</code></pre>
<p>Como vemos, tenemos que afinar los parámetros del algoritmo.</p>
</div>
</div>
<div id="modificaciones-de-gradient-boosting" class="section level2">
<h2><span class="header-section-number">12.8</span> Modificaciones de Gradient Boosting</h2>
<p>Hay algunas adiciones al algoritmo de gradient boosting que podemos usar para mejorar el desempeño. Los dos métodos que comunmente se usan son encogimiento (<em>shrinkage</em>), que es una especie de tasa de aprendizaje, y submuestreo, donde construimos cada árbol adicional usando una submuestra de la muestra de entrenamiento.</p>
<p>Ambas podemos verlas como técnicas de regularización, que limitan sobreajuste producido por el algoritmo agresivo de boosting.</p>
<div id="tasa-de-aprendizaje-shrinkage" class="section level3">
<h3><span class="header-section-number">12.8.1</span> Tasa de aprendizaje (shrinkage)</h3>
<p>Funciona bien modificar el algoritmo usando una tasa de aprendizae <span class="math inline">\(0&lt;\nu&lt;1\)</span>: <span class="math display">\[f_m(x) = f_{m-1}(x) + \nu \sum_j \gamma_{j,m} I(x\in R_{j,m})\]</span></p>
<p>Este parámetro sirve como una manera de evitar sobreajuste rápido cuando construimos los predictores. Si este número es muy alto, podemos sobreajustar rápidamente con pocos árboles, y terminar con predictor de varianza alta. Si este número es muy bajo, puede ser que necesitemos demasiadas iteraciones para llegar a buen desempeño.</p>
<p>Igualmente se prueba con varios valores de <span class="math inline">\(0&lt;\nu&lt;1\)</span> (típicamente <span class="math inline">\(\nu&lt;0.1\)</span>) para mejorar el desempeño en validación. <strong>Nota</strong>: cuando hacemos <span class="math inline">\(\nu\)</span> más chica, es necesario hacer <span class="math inline">\(M\)</span> más grande (correr más árboles) para obtener desempeño óptimo.</p>
<p>Veamos que efecto tiene en nuestro ejemplo:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modelos_dat &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">n_modelo =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>))
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">map</span>(shrinkage, boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
modelos_dat</code></pre></div>
<pre><code>## # A tibble: 4 x 4
##   n_modelo shrinkage    modelo                 eval
##      &lt;int&gt;     &lt;dbl&gt;    &lt;list&gt;               &lt;list&gt;
## 1        1      0.05 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt;
## 2        2      0.10 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt;
## 3        3      0.50 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt;
## 4        4      1.00 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(shrinkage, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
graf_eval</code></pre></div>
<pre><code>## # A tibble: 4,000 x 4
##    shrinkage n_arbol    tipo     valor
##        &lt;dbl&gt;   &lt;int&gt;   &lt;chr&gt;     &lt;dbl&gt;
##  1      0.05       1 entrena 0.3914774
##  2      0.05       2 entrena 0.3795092
##  3      0.05       3 entrena 0.3683144
##  4      0.05       4 entrena 0.3578551
##  5      0.05       5 entrena 0.3480027
##  6      0.05       6 entrena 0.3387286
##  7      0.05       7 entrena 0.3300476
##  8      0.05       8 entrena 0.3210405
##  9      0.05       9 entrena 0.3127008
## 10      0.05      10 entrena 0.3047083
## # ... with 3,990 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo<span class="op">==</span><span class="st">&#39;valida&#39;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(shrinkage), <span class="dt">group =</span>
                        shrinkage)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>tipo)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-18-1.png" width="480" /></p>
<p>Obsérvese que podemos obtener un mejor resultado de validación afinando la tasa de aprendizaje. Cuando es muy grande, el modelo rápidamente sobreajusta cuando agregamos árboles. Si la tasa es demasiado chica, podos tardar mucho en llegar a un predictor de buen desempeño.</p>
<p>¿Cómo crees que se ven las gráfica de error de entrenamiento?</p>
</div>
<div id="submuestreo-bag.fraction" class="section level3">
<h3><span class="header-section-number">12.8.2</span> Submuestreo (bag.fraction)</h3>
<p>Funciona bien construir cada uno de los árboles con submuestras de la muestra de entrenamiento, como una manera adicional de reducir varianza al construir nuestro predictor (esta idea es parecida a la de los bosques aleatorios, aquí igualmente perturbamos la muestra de entrenamiento en cada paso para evitar sobreajuste). Adicionalmente, este proceso acelera considerablemente las iteraciones de boosting, y en algunos casos sin penalización en desempeño.</p>
<p>En boosting generalmente se toman submuestras (una fracción de alrededor de 0.5 de la muestra de entrenamiento, pero puede ser más chica para conjuntos grandes de entrenamiento) sin reemplazo.</p>
<p>Este parámetro también puede ser afinado con muestra de validación o validación cruzada.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">boost &lt;-<span class="st"> </span><span class="kw">ajustar_boost</span>(entrena)
modelos_dat &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">n_modelo =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, 
                          <span class="dt">bag.fraction =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>),
                          <span class="dt">shrinkage =</span> <span class="fl">0.25</span>)
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">pmap</span>(., boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
modelos_dat</code></pre></div>
<pre><code>## # A tibble: 3 x 5
##   n_modelo bag.fraction shrinkage    modelo                 eval
##      &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;list&gt;               &lt;list&gt;
## 1        1         0.25      0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt;
## 2        2         0.50      0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt;
## 3        3         1.00      0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(bag.fraction, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
graf_eval</code></pre></div>
<pre><code>## # A tibble: 3,000 x 4
##    bag.fraction n_arbol    tipo     valor
##           &lt;dbl&gt;   &lt;int&gt;   &lt;chr&gt;     &lt;dbl&gt;
##  1         0.25       1 entrena 0.3404206
##  2         0.25       2 entrena 0.2948882
##  3         0.25       3 entrena 0.2582633
##  4         0.25       4 entrena 0.2316890
##  5         0.25       5 entrena 0.2106434
##  6         0.25       6 entrena 0.1946327
##  7         0.25       7 entrena 0.1825413
##  8         0.25       8 entrena 0.1733467
##  9         0.25       9 entrena 0.1652359
## 10         0.25      10 entrena 0.1596591
## # ... with 2,990 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>((graf_eval), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>tipo, <span class="dt">ncol =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-19-1.png" width="480" /></p>
<p>En este ejemplo, podemos reducir el tiempo de ajuste usando una fracción de submuestro de 0.5, con quizá algunas mejoras en desempeño.</p>
<p>Ahora veamos los dos parámetros actuando en conjunto:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modelos_dat &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">bag.fraction =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>),
                          <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>)) <span class="op">%&gt;%</span><span class="st"> </span>expand.grid
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">pmap</span>(., boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(shrinkage, bag.fraction, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
<span class="kw">head</span>(graf_eval)</code></pre></div>
<pre><code>##   shrinkage bag.fraction n_arbol    tipo     valor
## 1      0.01          0.1       1 entrena 0.4016655
## 2      0.01          0.1       2 entrena 0.3991252
## 3      0.01          0.1       3 entrena 0.3964301
## 4      0.01          0.1       4 entrena 0.3942135
## 5      0.01          0.1       5 entrena 0.3914665
## 6      0.01          0.1       6 entrena 0.3891097</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>shrinkage)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-20-1.png" width="480" /></p>
<p>Bag fraction demasiado chico no funciona bien, especialmente si la tasa de aprendizaje es alta (¿Por qué?). Filtremos para ver con detalle el resto de los datos:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>, bag.fraction<span class="op">&gt;</span><span class="fl">0.1</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>shrinkage) <span class="op">+</span><span class="st"> </span><span class="kw">scale_y_log10</span>()</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-21-1.png" width="480" /></p>
<p>Y parece ser que para este número de iteraciones, una tasa de aprendizaje de 0.1 junto con un bag fraction de 0.5 funciona bien:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">graf_eval <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(tipo<span class="op">==</span><span class="st">&#39;valida&#39;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(shrinkage, bag.fraction) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">valor =</span> <span class="kw">min</span>(valor)) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">arrange</span>(valor) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>(<span class="dv">10</span>)</code></pre></div>
<pre><code>## # A tibble: 10 x 3
## # Groups:   shrinkage [3]
##    shrinkage bag.fraction     valor
##        &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;
##  1      0.10         0.50 0.1246740
##  2      0.25         0.50 0.1283885
##  3      0.10         1.00 0.1305941
##  4      0.10         0.25 0.1310472
##  5      0.01         0.25 0.1311681
##  6      0.10         0.10 0.1320537
##  7      0.25         0.25 0.1333946
##  8      0.25         1.00 0.1349612
##  9      0.01         0.50 0.1366676
## 10      0.01         0.10 0.1367777</code></pre>
</div>
<div id="numero-de-arboles-m" class="section level3">
<h3><span class="header-section-number">12.8.3</span> Número de árboles M</h3>
<p>Se monitorea el error sobre una muestra de validación cuando agregamos cada árboles. Escogemos el número de árboles de manera que minimize el error de validación. Demasiados árboles pueden producir sobreajuste. Ver el ejemplo de arriba.</p>
</div>
<div id="tamano-de-arboles" class="section level3">
<h3><span class="header-section-number">12.8.4</span> Tamaño de árboles</h3>
<p>Los árboles se construyen de tamaño fijo <span class="math inline">\(J\)</span>, donde <span class="math inline">\(J\)</span> es el número de cortes. Usualmente <span class="math inline">\(J=1,2,\ldots, 10\)</span>, y es un parámetro que hay que elegir. <span class="math inline">\(J\)</span> más grande permite interacciones de orden más alto entre las variables de entrada. Se intenta con varias <span class="math inline">\(J\)</span> y <span class="math inline">\(M\)</span> para minimizar el error de vaidación.</p>
</div>
<div id="controlar-numero-de-casos-para-cortes" class="section level3">
<h3><span class="header-section-number">12.8.5</span> Controlar número de casos para cortes</h3>
<p>Igual que en bosques aleatorios, podemos establecer mínimos de muestra en nodos terminales, o mínimo de casos necesarios para hacer un corte.</p>
</div>
<div id="ejemplo-47" class="section level3 unnumbered">
<h3>Ejemplo</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modelos_dat &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">bag.fraction =</span> <span class="kw">c</span>( <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>),
                          <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>),
                    <span class="dt">depth =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">12</span>)) <span class="op">%&gt;%</span><span class="st"> </span>expand.grid
modelos_dat &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modelo =</span> <span class="kw">pmap</span>(., boost)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eval =</span> <span class="kw">map</span>(modelo, eval_modelo))
graf_eval &lt;-<span class="st"> </span>modelos_dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(shrinkage, bag.fraction, depth, eval) <span class="op">%&gt;%</span><span class="st"> </span>unnest
<span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(depth<span class="op">~</span>shrinkage) <span class="op">+</span><span class="st"> </span><span class="kw">scale_y_log10</span>()</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-23-1.png" width="480" /></p>
<p>Podemos ver con más detalle donde ocurre el mejor desempeño:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">filter</span>(graf_eval, tipo <span class="op">==</span><span class="st">&#39;valida&#39;</span>, shrinkage <span class="op">==</span><span class="st"> </span><span class="fl">0.1</span>, n_arbol<span class="op">&gt;</span><span class="dv">100</span>), <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span> valor, <span class="dt">colour=</span><span class="kw">factor</span>(bag.fraction), <span class="dt">group =</span>
                        bag.fraction)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(depth<span class="op">~</span>shrinkage) </code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-24-1.png" width="480" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">arrange</span>(<span class="kw">filter</span>(graf_eval,tipo<span class="op">==</span><span class="st">&#39;valida&#39;</span>), valor))</code></pre></div>
<pre><code>##   shrinkage bag.fraction depth n_arbol   tipo     valor
## 1       0.1          0.5    10      98 valida 0.1218075
## 2       0.1          0.5    10      95 valida 0.1218644
## 3       0.1          0.5    10      97 valida 0.1218835
## 4       0.1          0.5    10      96 valida 0.1219046
## 5       0.1          0.5    10     100 valida 0.1220013
## 6       0.1          0.5    10      94 valida 0.1220148</code></pre>
</div>
<div id="evaluacion-con-validacion-cruzada." class="section level3">
<h3><span class="header-section-number">12.8.6</span> Evaluación con validación cruzada.</h3>
<p>Para datos no muy grandes, conviene escoger modelos usando validación cruzada.</p>
<p>Por ejemplo,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">9983</span>)
<span class="kw">rm</span>(<span class="st">&#39;modelos_dat&#39;</span>)
mod_boosting &lt;-<span class="st"> </span><span class="kw">gbm</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span>.,  <span class="dt">data =</span> entrena,
                <span class="dt">distribution =</span> <span class="st">&#39;gaussian&#39;</span>,
                <span class="dt">n.trees =</span> <span class="dv">200</span>, 
                <span class="dt">interaction.depth =</span> <span class="dv">10</span>,
                <span class="dt">shrinkage =</span> <span class="fl">0.1</span>, <span class="co"># tasa de aprendizaje</span>
                <span class="dt">bag.fraction =</span> <span class="fl">0.5</span>,
                <span class="dt">cv.folds =</span> <span class="dv">10</span>)
<span class="kw">gbm.perf</span>(mod_boosting)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">eval_modelo_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="cf">function</span>(modelo){
   dat_eval &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">entrena =</span> <span class="kw">sqrt</span>(modelo<span class="op">$</span>train.error),
                          <span class="dt">valida =</span> <span class="kw">sqrt</span>(modelo<span class="op">$</span>cv.error),
                          <span class="dt">n_arbol =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(modelo<span class="op">$</span>train.error)) <span class="op">%&gt;%</span>
<span class="st">                      </span><span class="kw">gather</span>(tipo, valor, <span class="op">-</span>n_arbol)
   dat_eval
}
dat &lt;-<span class="st"> </span><span class="kw">eval_modelo_2</span>(mod_boosting)
<span class="kw">sqrt</span>(<span class="kw">min</span>(mod_boosting<span class="op">$</span>cv.error))
<span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x =</span> n_arbol, <span class="dt">y=</span>valor, <span class="dt">colour=</span>tipo, <span class="dt">group=</span>tipo)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</code></pre></div>
</div>
</div>
<div id="graficas-de-dependencia-parcial" class="section level2">
<h2><span class="header-section-number">12.9</span> Gráficas de dependencia parcial</h2>
<p>La idea de dependencia parcial que veremos a continuación se puede aplicar a cualquier método de aprendizaje, y en boosting ayuda a entender el funcionamiento del predictor complejo que resulta del algoritmo. Aunque podemos evaluar el predictor en distintos valores y observar cómo se comporta, cuando tenemos varias variables de entrada este proceso no siempre tiene resultados muy claros o completos. Dependencia parcial es un intento por entender de manera más sistemática parte del funcionamiento de un modelo complejo.</p>
<div id="dependencia-parcial" class="section level3">
<h3><span class="header-section-number">12.9.1</span> Dependencia parcial</h3>
<p>Supongamos que tenemos un predictor <span class="math inline">\(f(x_1,x_2)\)</span> que depende de dos variables de entrada. Podemos considerar la función <span class="math display">\[{f}_{1}(x_1) = E_{x_2}[f(x_1,x_2)],\]</span> que es el promedio de <span class="math inline">\(f(x)\)</span> fijando <span class="math inline">\(x_1\)</span> sobre la marginal de <span class="math inline">\(x_2\)</span>. Si tenemos una muestra de entrenamiento, podríamos estimarla promediando sobre la muestra de entrenamiento</p>
<p><span class="math display">\[\bar{f}_1(x_1) = \frac{1}{n}\sum_{i=1}^n f(x_1, x_2^{(i)}),\]</span> que consiste en fijar el valor de <span class="math inline">\(x_1\)</span> y promediar sobre todos los valores de la muestra de entrenamiento para <span class="math inline">\(x_2\)</span>.</p>
</div>
<div id="ejemplo-48" class="section level3 unnumbered">
<h3>Ejemplo</h3>
<p>Construimos un modelo con solamente tres variables para nuestro ejemplo anterior</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">gbm</span>(<span class="kw">log</span>(vSalePrice) <span class="op">~</span><span class="st"> </span>vGrLivArea <span class="op">+</span>vNeighborhood  <span class="op">+</span>vOverallQual,  
                <span class="dt">data =</span> entrena,
                <span class="dt">distribution =</span> <span class="st">&#39;gaussian&#39;</span>,
                <span class="dt">n.trees =</span> <span class="dv">100</span>, 
                <span class="dt">interaction.depth =</span> <span class="dv">3</span>,
                <span class="dt">shrinkage =</span> <span class="fl">0.1</span>, 
                <span class="dt">bag.fraction =</span> <span class="fl">0.5</span>,
                <span class="dt">train.fraction =</span> <span class="fl">0.75</span>)
<span class="kw">gbm.perf</span>(mod_<span class="dv">2</span>)</code></pre></div>
<pre><code>## Using test method...</code></pre>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-26-1.png" width="480" /></p>
<pre><code>## [1] 87</code></pre>
<p>Podemos calcular a mano la gráfica de dependencia parcial para el tamaño de la “General Living Area”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat_dp &lt;-<span class="st"> </span>entrena <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(vGrLivArea, vNeighborhood, vOverallQual) </code></pre></div>
<p>Consideramos el rango de la variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cuantiles &lt;-<span class="st"> </span><span class="kw">quantile</span>(entrena<span class="op">$</span>vGrLivArea, <span class="dt">probs=</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>))
cuantiles</code></pre></div>
<pre><code>##     0%    10%    20%    30%    40%    50%    60%    70%    80%    90% 
##  334.0  912.0 1066.6 1208.0 1339.0 1464.0 1578.0 1709.3 1869.0 2158.3 
##   100% 
## 5642.0</code></pre>
<p>Por ejemplo, vamos evaluar el efecto parcial cuando vGrLivArea = 912. Hacemos</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span> &lt;-<span class="st"> </span>dat_dp <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">vGrLivArea =</span> <span class="dv">912</span>) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod_<span class="dv">2</span>, .)) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">summarise</span>(<span class="dt">mean_pred =</span> <span class="kw">mean</span>(pred))</code></pre></div>
<pre><code>## Using 87 trees...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span></code></pre></div>
<pre><code>##   mean_pred
## 1  11.84206</code></pre>
<p>Evaluamos en vGrLivArea = 912</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span> &lt;-<span class="st"> </span>dat_dp <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">vGrLivArea =</span> <span class="dv">1208</span>) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod_<span class="dv">2</span>, .)) <span class="op">%&gt;%</span>
<span class="st">            </span><span class="kw">summarise</span>(<span class="dt">mean_pred =</span> <span class="kw">mean</span>(pred))</code></pre></div>
<pre><code>## Using 87 trees...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat_dp_<span class="dv">1</span></code></pre></div>
<pre><code>##   mean_pred
## 1  11.96576</code></pre>
<p>(un incremento de alrededor del 10% en el precio de venta). Hacemos todos los cuantiles como sigue:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cuantiles &lt;-<span class="st"> </span><span class="kw">quantile</span>(entrena<span class="op">$</span>vGrLivArea, <span class="dt">probs=</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))

prom_parcial &lt;-<span class="st"> </span><span class="cf">function</span>(x, variable, df, mod){
  variable &lt;-<span class="st"> </span><span class="kw">enquo</span>(variable)
  variable_nom &lt;-<span class="st"> </span><span class="kw">quo_name</span>(variable)
  salida &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="op">!!</span>variable_nom <span class="op">:</span><span class="er">=</span><span class="st"> </span>x) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod, ., <span class="dt">n.trees=</span><span class="dv">100</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(<span class="op">!!</span>variable) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarise</span>(<span class="dt">f_1 =</span> <span class="kw">mean</span>(pred)) 
  salida
}
dep_parcial &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(cuantiles, 
                       <span class="op">~</span><span class="kw">prom_parcial</span>(.x, vGrLivArea, entrena, mod_<span class="dv">2</span>))
<span class="kw">ggplot</span>(dep_parcial, <span class="kw">aes</span>(<span class="dt">x=</span>vGrLivArea, <span class="dt">y=</span> f_<span class="dv">1</span>)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_rug</span>(<span class="dt">sides=</span><span class="st">&#39;b&#39;</span>)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-31-1.png" width="480" /> Y transformando a las unidades originales</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dep_parcial, <span class="kw">aes</span>(<span class="dt">x=</span>vGrLivArea, <span class="dt">y=</span> <span class="kw">exp</span>(f_<span class="dv">1</span>))) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_rug</span>(<span class="dt">sides=</span><span class="st">&#39;b&#39;</span>)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-32-1.png" width="480" /> Y vemos que cuando aumenta el area de habitación, aumenta el precio. Podemos hacer esta gráfica más simple haciendo</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="dv">1</span>) <span class="co"># 1 pues es vGrLivArea la primer variable </span></code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-33-1.png" width="480" /></p>
<p>Y para una variable categórica se ve como sigue:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">return.grid =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(y)</code></pre></div>
<pre><code>##    vNeighborhood        y
## 1         IDOTRR 11.74437
## 2         BrDale 11.79033
## 3        OldTown 11.81014
## 4          SWISU 11.85419
## 5        MeadowV 11.85748
## 6        Edwards 11.86859
## 7        BrkSide 11.91787
## 8          Otros 11.95479
## 9          NAmes 11.98455
## 10        Sawyer 12.00362
## 11       SawyerW 12.03340
## 12       Mitchel 12.06104
## 13        NWAmes 12.08210
## 14       Crawfor 12.08735
## 15       Gilbert 12.09874
## 16       Blmngtn 12.11353
## 17       CollgCr 12.12275
## 18       Somerst 12.15674
## 19        Timber 12.17366
## 20       ClearCr 12.18768
## 21       NoRidge 12.20368
## 22       StoneBr 12.22289
## 23       NridgHt 12.24190
## 24       Veenker 12.25304</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">return.grid =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-34-1.png" width="480" /></p>
<hr />
<p>En general, si nuestro predictor depende de más variables <span class="math inline">\(f(x_1,x_2, \ldots, x_p)\)</span> entrada. Podemos considerar las funciones <span class="math display">\[{f}_{j}(x_j) = E_{(x_1,x_2, \ldots x_p) - x_j}[f(x_1,x_2, \ldots, x_p)],\]</span> que es el valor esperado de <span class="math inline">\(f(x)\)</span> fijando <span class="math inline">\(x_j\)</span>, y promediando sobre el resto de las variables. Si tenemos una muestra de entrenamiento, podríamos estimarla promediando sobre la muestra de entrenamiento</p>
<p><span class="math display">\[\bar{f}_j(x_j) = \frac{1}{n}\sum_{i=1}^n f(x_1^{(i)}, x_2^{(i)}, \ldots, x_{j-1}^{(i)},x_{j+1}^{(i)},\ldots, x_p^{(i)}).\]</span></p>
<p>Podemos hacer también gráficas de dependencia parcial para más de una variable, si fijamos un subconjunto de variables y promediamos sobre el resto.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mod_<span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</code></pre></div>
<p><img src="12-arboles-2_files/figure-html/unnamed-chunk-35-1.png" width="480" /></p>
</div>
<div id="discusion-2" class="section level3">
<h3><span class="header-section-number">12.9.2</span> Discusión</h3>
<p>En primer lugar, veamos qué obtenemos de la dependencia parcial cuando aplicamos al modelo lineal sin interacciones. En el caso de dos variables,</p>
<p><span class="math display">\[f_1(x_1) = E_{x_2}[f(x_1,x_2)] =E_{x_2}[a + bx_1 + cx_2)] = \mu + bx_1,\]</span> que es equivalente al análisis marginal que hacemos en regresión lineal ( incrementos en la variable <span class="math inline">\(x_1\)</span> con todo lo demás fijo, donde el incremento marginal de la respuesta es el coeficiente <span class="math inline">\(b\)</span>).</p>
<p>Desde este punto de vista, dependencia parcial da una interpretación similar a la del análisis usual de coeficientes en regresión lineal, donde pensamos en “todo lo demás constante”.</p>
<p>Nótese también que cuando hay <strong>interacciones</strong> fuertes entre las variables, ningún análisis marginal (dependencia parcial o examen de coeficientes) da un resultado fácilmente interpretable - la única solución es considerar el efecto conjunto de las variables que interactúan. De modo que este tipo de análisis funciona mejor cuando no hay interacciones grandes entre las variables (es cercano a un modelo aditivo con efectos no lineales).</p>
<div id="ejemplo-49" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Considera qué pasa con las gráficas de dependencia parcial cuando <span class="math inline">\(f(x_1,x_2) = -10 x_1x_2\)</span>, y <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> tienen media cero. Explica por qué en este caso es mejor ver el efecto conjunto de las dos variables.</p>
<hr />
<p>Es importante también evitar la interpretación incorrecta de que la función de dependencia parcial da el valor esperado del predictor condicionado a valores de la variable cuya dependencia examinamos. Es decir, <span class="math display">\[f_1(x_1) = E_{x_2}(f(x_1,x_2)) \neq E(f(x_1,x_2)|x_1).\]</span> La última cantidad es un valor esperado diferente (calculado sobre la condicional de <span class="math inline">\(x_2\)</span> dada <span class="math inline">\(x_1\)</span>), de manera que utiliza información acerca de la relación que hay entre <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span>. La función de dependencia parcial da el efecto de <span class="math inline">\(x_1\)</span> tomando en cuenta los efectos promedio de las otras variables.</p>
</div>
</div>
</div>
<div id="tarea-5" class="section level2 unnumbered">
<h2>Tarea</h2>
<ol style="list-style-type: decimal">
<li>Revisa el script que vimos en clase de aplicación de bosques para predecir precios de casa (bosque-housing.Rmd). Argumenta por qué es mejor el segundo método para limpiar faltantes que el primero. Considera</li>
</ol>
<ul>
<li>Cómo respeta cada método la división entrenamiento y validación</li>
<li>El desempeño de cada método</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><p>Considera las importancia de variables de bosque-housing.Rmd. Muestra las importancias basadas en permutaciones escaladas y no escaladas. ¿Con qué valores en el objeto randomForest se escalan las importancias?</p></li>
<li><p>Grafica importancias de Gini (MeanDecreaseGini) y de permutaciones. ¿Los resultados son similiares? Explica qué significa MeanDecreaseGini en el contexto de un problema de regresión.</p></li>
<li><p>Considera nuestra primera corrida de gradient boosting en las notas para el ejemplo de los precios de las casas. Corre este ejemplo usando pérdida absoluta (<span class="math inline">\(|y-f(x)|\)</span>) en lugar de pérdida cuadrática (<span class="math inline">\((y-f(x))^2\)</span>)</p></li>
</ol>
<ul>
<li>Grafica las curvas de entrenamiento y validación conforme se agregan árboles</li>
<li>Explica teóricamente cuál es la diferencia del algoritmo cuando utilizas estas dos pérdidas.</li>
<li>Da razones por las que pérdida absoluta puede ser una mejor selección para algunos problemas de regresión.</li>
</ul>

<div id="refs" class="references">
<div>
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning (Information Science and Statistics)</em>. Secaucus, NJ, USA: Springer-Verlag New York, Inc.</p>
</div>
<div>
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.</p>
</div>
<div>
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. <em>The Elements of Statistical Learning</em>. Springer Series in Statistics. Springer New York Inc. <a href="http://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">http://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
<div>
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. <em>An Introduction to Statistical Learning: With Applications in R</em>. Springer Publishing Company, Incorporated. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
<div>
<p>Ng, Andrew. 2017. “Machine Learning.” <a href="https://www.coursera.org/learn/machine-learning" class="uri">https://www.coursera.org/learn/machine-learning</a>.</p>
</div>
<div>
<p>Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>J. Mach. Learn. Res.</em> 15 (1). JMLR.org: 1929–58. <a href="http://dl.acm.org/citation.cfm?id=2627435.2670313" class="uri">http://dl.acm.org/citation.cfm?id=2627435.2670313</a>.</p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ESL">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2017. <em>The Elements of Statistical Learning</em>. Springer Series in Statistics. Springer New York Inc. <a href="http://web.stanford.edu/~hastie/ElemStatLearn/" class="uri">http://web.stanford.edu/~hastie/ElemStatLearn/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="metodos-basados-en-arboles.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-2017/edit/master/12-arboles-2.Rmd",
"text": "Edit"
},
"download": ["aprendizaje-maquina.pdf", "aprendizaje-maquina.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
