# Regularización

Los métodos para ajustar modelos lineales que vimos en secciones anteriores
(mínimos cuadrados y minimización de devianza)

## Sesgo y varianza de predictores


Consideremos el problema de regresión, donde el proceso que genera los
datos está dado por
$$Y = f(X) + \epsilon$$

Consideremos que queremos hacer predicciones para una $X=x_0$ particular, de modo
que el error es

$$Y - \hat{f}(x_0) = (f(x_0) - \hat{f}(x_0)) + \epsilon$$
Como discutimos antes, no podemos hacer nada por la variación de $\epsilon$.
La pregunta es entonces ¿por qué podría pasar que $\hat{f}(x_0)$ estuviera lejos
de $\hat{f}(x_0)$? Recordemos que $\hat{f}(x_0)$ depende de una muestra
de entrenamiento ${\mathcal L}$, de modo que:

- Puede ser que $\hat{f}(x_0)$ está consistentemente lejos de $f(x_0)$, independientemente
de cuál es la muestra de entrenamiento.
- Puede ser que $\hat{f}(x_0)$ varía mucho dependiendo de la muestra de entrenamiento,
y en consecuencia es poco probable que $\hat{f}(x_0)$ esté cerca de  $f(x_0)$.

Es posible demostrar sin mucha dificultad que 

$$E\left ( (f(x_0)-\hat{f}(x_0))^2   \right) =
(f(x_0) - E(\hat{f}(x_0)))^2 + Var (\hat{f}(x_0))$$

Al primer término le llamamos **sesgo** : Qué tan lejos en promedio están las estimaciones
de nuestro modelo del verdadero valor, y al segundo término le llamamos
**varianza**: qué tanto varían las estimaciones del modelo. Ambas pueden
ser razones por las que obtengamos predicciones malas.


#### Ejemplo {-}

Consideremos dos métodos: regresión lineal y regresión polinomial (pensemos
que es un tipo de ajuste de curvas). Para ilustrar los conceptos de sesgo
y varianza simularemos varios posibles muestras de entrenamiento:

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
```

```{r}
f <- function(x){ sin(6*x)}
sim_data <- function(n = 15){
  x <- runif(n, 0, 1)
  y <- f(x) + rnorm(n, 0, 0.4)
  data_frame(x = x, y = y)
}
dat <- sim_data(n = 100)
plot(dat$x,dat$y)
```

```{r}
set.seed(92114)
sims <- data_frame(rep = 1:10)
sims <- sims %>% group_by(rep) %>% 
  mutate(data = list(data = sim_data())) %>% unnest
```

Regresión lineal en $x$ nos da diferencias consistentes entre predicciones
y observaciones (es un método que sufre de sesgo):

```{r}
ggplot(sims, aes(x=x, y=y)) + geom_point() +
  facet_wrap(~rep) + geom_smooth(formula = y~x, method ='lm', colour = 'red', se = FALSE) +
  ylim(c(-3,3))
```

Mientras que regresión polinomial nos da diferencias variables y grandes
entre predicciones y observaciones (es un método que sufre de varianza):

```{r}
ggplot(sims, aes(x=x, y=y)) + geom_point() +
  facet_wrap(~rep) + geom_smooth(formula = y~ poly(x, 5, raw = TRUE), method ='lm', 
                                 colour = 'red', se = FALSE) + ylim(c(-3,3))
```

En este ejemplo, ambos métodos se desempeñan mal, pero por razones distintas.
El primer método sufre de sesgo: es un método rígido que no aprende de patrones
en los datos. El segundo método sufre de varianza: es un método flexible que aprende
ruido. Cada uno de estos problemas requiere soluciones diferentes. 




### Sesgo y varianza en modelos lineales

Aunque típicamente pensamos que los modelos lineales son métodos simples, con
estructura rígida, y que tienden a sufrir más por sesgo que por varianza (parte de 
la razón por la que existen métodos más flexibles como bosques aleatorios, redes
nueronales, etc.), hay varias razones por las que los métodos lineales pueden sufrir
de varianza alta:

- Cuando la muestra de entrenamiento es relativamente chica ($N$ chica), la varianza
puede ser alta.

- Cuando el número de entradas  $p$ es grande, podemos también sufrir de varianza grande
(pues tenemos muchos parámetros para estimar).

- Cuando hay variables correlacionadas en las entradas la varianza también puede ser alta.


En estos casos, conviene buscar maneras de reducir varianza - generalmente a costa
de un incremento de sesgo.

#### Ejemplo {-}


Consideramos regresión logística. En primer lugar, supondremos que 
tenemos un problema con $n=400$ y $p=100$, y tomamos como modelo para los datos (sin 
ordenada al origen):

$$p_1(x)=h\left(\sum_{j=1}^{100} \beta_j x_j\right ),$$


donde $h$ es la función logística. 
Nótese que este es el {\em verdadero modelo para los datos}. Para producir datos
de entrenamiento, primero generamos las betas fijas, y después, utilizando estas betas,
generamos 400 casos de entrenamiento.

Generamos las betas:

```{r}
h <- function(x){ exp(x) / (1 + exp(x))}
set.seed(2805)
beta <- rnorm(100,0,0.1)
beta
```

Con esta función simulamos datos de entrenamiento (400) y datos
de prueba (5000).

```{r}
sim_1 <- function(n, m, beta){
  p <- length(beta)
  #n = casos de entrenamiento, m= casos de prueba, p=num variables
  mat.entrena <- matrix(rnorm(n*p, 0, 0.5), n, p) + rnorm(p)
  mat.prueba <- matrix(rnorm(n*p, 0, 0.5), n, p) + rnorm(p)
  g.entrena = rbinom(n, 1, h(mat.entrena%*%beta))
  g.prueba = rbinom(m, 1, h(mat.prueba%*%beta))
  list(mat.entrena = mat.entrena, mat.prueba = mat.prueba, g.entrena=g.entrena, 
    g.prueba=g.prueba)
}
salida <- sim_1(n=400, m=2000, beta = beta)
names(salida)
```

Y ahora ajustamos el modelo de regresión logística:

```{r}
mod.1 <- glm.fit(x = salida$mat.entrena,
                 y = salida$g.entrena, family = binomial())
```

¿Qué tan buenas fueron nuestras estimaciones?

```{r}
qplot(beta, mod.1$coefficients) + 
  xlab('Coeficientes') + 
  ylab('Coeficientes estimados') +
  geom_abline(xintercept=0, slope =1) +
  xlim(c(-1,1))+ ylim(c(-1,1))
```

Y notamos que las estimaciones no son muy buenas.
Podemos hacer otra simulación para confirmar que el problema
es que las estimaciones son muy variables.

Con otra muestra de entrenamiento:
```{r}
salida.2 <- sim_1(n=400, m=10, beta = beta)
names(salida.2)
mod.2 <- glm.fit(x = salida.2$mat.entrena,
                 y = salida.2$g.entrena, family = binomial())
qplot(beta, mod.2$coefficients) + xlab('Coeficientes') + 
  ylab('Coeficientes estimados') +
  geom_abline(xintercept=0, slope =1) +
  xlim(c(-1,1))+ ylim(c(-1,1))
```

Si repetimos varias veces:
```{r}
dat.sim <- plyr::ldply(1:50, function(i){
  salida.2 <- sim_1(n=400, m=10, beta)
  mod.2 <- glm.fit(x = salida.2$mat.entrena,
                 y = salida.2$g.entrena, family = binomial())
  data.frame(rep=i, vars=1:length(coef(mod.2)), coefs=coef(mod.2))
})
```

Vemos que hay mucha variabilidad en la estimación de los coeficientes
 (en rojo están los verdaderos):

```{r}
names(beta) <- 1:length(beta)
dat.sim$vars <- reorder(as.character(dat.sim$vars), dat.sim$coefs, median)
ggplot(dat.sim, aes(x=vars, y=coefs)) + geom_boxplot() +
  geom_line(data=data.frame(coefs=beta, vars=names(beta)), 
    aes(y=beta, group=1), col='red',size=1.1) + coord_flip()
```

En la práctica, nosotros tenemos una sola muestra de entrenamiento.
Así que, con una muestra de tamaño $n=500$ como en este ejemplo,
obtendremos típicamente resultados no muy buenos. {\bf Estos
coeficientes ruidosos afectan nuestras predicciones de manera negativa}.

Vemos ahora lo que pasa con nuestra $\hat{p}_1(x)$ estimadas, comparándolas
con $p_1(x)$, para la primera simulación:

```{r}
dat.entrena <- data.frame(prob.hat.1=h(mod.1$fitted.values), prob.1=h(salida$mat.entrena%*%beta),
  clase = salida$g.entrena)
dat.prueba <- data.frame(prob.hat.1=h(salida$mat.prueba%*%(mod.1$coefficients)), 
  prob.1=h(salida$mat.prueba%*%beta),
  clase = salida$g.prueba)
```
Para los datos de entrenamiento:
```{r}
ggplot(dat.entrena, aes(x=prob.1, y=prob.hat.1, colour=factor(clase))) + geom_point()
```

Y con la muestra de prueba:
```{r}
ggplot(dat.prueba, aes(x=prob.1, y=prob.hat.1, colour=factor(clase))) + geom_point()
```


Si la estimación fuera perfecta, 
esta gráfica sería una diagonal. Vemos entonces
que cometemos errores grandes. El problema no es que nuestro modelo no sea apropiado
(logístico), pues ese es el modelo real. El problema es la variabilidad en la estimación
de los coeficientes que notamos arriba.


La matriz de confusión y la sensibilidad y especificidad:
```{r}
table(dat.prueba$prob.hat.1>0.5,dat.prueba$clase)
prop.table(table(dat.prueba$prob.hat.1>0.5,dat.prueba$clase), margin=2)
```



Si el problema es la varianza, podemos atacar este problema
poniendo restricciones a los coeficientes, de manera que caigan en rangos 
más aceptables.  Una manera de hacer esto es sustituir el problema de minimización
de regresión logística, que es minimizar la devianza:

$$\min_{\beta} D(\beta)$$

con un problema penalizado

$$\min_{\beta} D(\beta) + \lambda\sum_{i=1}^p \beta_j^2$$

escogiendo un valor apropiado de $\lambda$.


En este caso obtenemos (veremos más del paquete {\em glmnet}):

```{r}
library(glmnet)
mod.restringido <- glmnet(x=salida$mat.entrena, y=salida$g.entrena, 
  alpha = 0,
  family='binomial', intercept = F, 
  lambda = 0.1)
beta.restr <- coef(mod.restringido)[-1]
sum(beta.restr^2)
```

Los nuevos coeficientes estimados:
```{r}
qplot(beta, beta.restr) + 
  xlab('Coeficientes') + 
  ylab('Coeficientes estimados') +
  geom_abline(xintercept=0, slope =1) +
  xlim(c(-0.5,0.5))+ ylim(c(-0.5,0.5))
```


```{r}
dat.entrena.r <- data.frame(prob.hat.1= h(salida$mat.entrena%*%as.numeric(beta.restr)), 
  prob.1=h(salida$mat.entrena%*%beta),
  clase = salida$g.entrena)
dat.prueba.r <- data.frame(prob.hat.1=h(salida$mat.prueba%*%as.numeric(beta.restr)), 
  prob.1=h(salida$mat.prueba%*%beta),
  clase = salida$g.prueba)
```
Para los datos de entrenamiento:
```{r}
ggplot(dat.entrena.r, aes(x=prob.1, y=prob.hat.1, colour=factor(clase))) + geom_point()
```

Y con la muestra de prueba:
```{r}
ggplot(dat.prueba.r, aes(x=prob.1, y=prob.hat.1, colour=factor(clase))) + geom_point()
```


```{r}
table(dat.prueba.r$prob.hat.1>0.5, dat.prueba.r$clase)
prop.table(table(dat.prueba.r$prob.hat.1>0.5, dat.prueba.r$clase), margin=2)
```

Curvas ROC de prueba:

```{r}
library(ROCR)
pred <- prediction(predictions= dat.prueba$prob.hat.1, labels = dat.prueba$clase)
perf <- performance(pred, measure = "sens", x.measure = "fpr") 
plot(perf)
pred.r <- prediction(predictions= dat.prueba.r$prob.hat.1, labels = dat.prueba.r$clase)
perf.r <- performance(pred.r, measure = "sens", x.measure = "fpr") 
plot(perf.r, add =T, col ='red')
abline(a=0, b=1, col ='gray')
```


Sin embargo, vemos que en la muestra de entrenamiento se desempeña mejor
el modelo sin restricciones, como es des esperarse (el mínimo irrestricto es
más bajo que el mínimo del problema con restricción).


```{r}
library(ROCR)
pred <- prediction(predictions= dat.entrena$prob.hat.1, labels = dat.entrena$clase)
perf <- performance(pred, measure = "sens", x.measure = "fpr") 
plot(perf)
pred.r <- prediction(predictions= dat.entrena.r$prob.hat.1, labels = dat.entrena.r$clase)
perf.r <- performance(pred.r, measure = "sens", x.measure = "fpr") 
plot(perf.r, add =T, col ='red')
abline(a=0, b=1, col ='gray')
```



## Regularización ridge

rriba vimos un ejemplo de regresión penalizada tipo **ridge**. Recordemos
que para regresión lineal, buscábamos minimizar la cantidad
$$D(\beta)=\frac{1}{n}\sum_{i=1}^n (y_i -\beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2$$
y en regresión logística,
$$D(\beta)=-\frac{2}{n}\sum_{i=1}^n y_i log(h(\beta_0 + \sum_{j=1}^p \beta_j x_{ij})) + (1-y_i) log(1 - h(\beta_0 + \sum_{j=1}^p \beta_j x_{ij}))    ,$$
donde los denotamos de la misma forma para unificar notación.

```{block2, type='comentario'}
En regresión **ridge** (lineal/logística), para $\lambda>0$ fija minimizamos
$$D_{\lambda}^2 (\beta)=D(\beta)  + \lambda\sum_{i=1}^p \beta_j^2$$,
donde suponemos que las entradas están estandarizadas (centradas y escaladas por
la desviación estándar).
```

#### Observaciones {-}

- En lugar de restrigir los coeficientes como hicimios en el ejemplo
de arriba, **penalizamos** la devianza. El efecto es el mismo: cuando intentamos
minimizar la devianza penalizada, parte del esfuerzo está en hacer $D(\beta)$ chico, 
sin hacer demasiado grandes los coeficientes para que el término de penalización
sea chico.
- La idea de regresión penalizada consiste en estabilizar la estimación de los
coeficientes, especialmente en casos donde tenemos muchas variables en relación
a los casos de entrenamiento. La penalización no permite que varíen tan fuertemente
los coeficientes.
- Cuando $\lambda$ es mas grande, los coeficientes se encogen más fuertemente
hacia cero con respecto al problema no regularizado. En este caso, estamos
**reduciendo la varianza** pero potencialmente **incrementando el sesgo**.
- Cuando $\lambda$ es mas chico, los coeficientes se encogen menos fuertemente
hacia cero, y quedan más cercanos a los coeficientes de mínimos cuadrados/máxima verosimilitud. 
En este caso, estamos
**reduciendo el sesgo** pero **incrementando la varianza**.
- Nótese que no penalizamos $\beta_0$. Es posible hacerlo, pero típicamente
no lo hacemos. En regresión lineal, de esta forma garantizamos que 
la predicción $\hat{y}$, cuando todas las variables $x_j$ toman su valor
en la media, es el promedio de las $y_i$'s de entrenamiento. Igualmente en 
regresión logística, la probabilidad ajustada cuando las entradas toman su
valor en la media es igual a $h(\beta_0)$.
- Que las variables estén estandarizadas es importante para que tenga
sentido la penalización. Si las variables $x_j$ están en distintas escalas (por ejemplo
pesos y dólares), entonces también los coeficientes $\beta_j$ están en distintas escalas,
y una penalización fija no afecta de la misma forma a cada coeficiente.


Resolver este problema por descenso en gradiente no tienen dificultad, pues:

```{block2, type='comentario'}
$$\frac{\partial D_{\lambda}^2 (\beta)}{\partial\beta_j} = \frac{\partial D(\beta)}{\beta_j} + 2\beta_j$$
para $j=1,\ldots, p$, y 
$$\frac{\partial D_{\lambda}^2 (\beta)}{\partial\beta_0} = \frac{\partial D(\beta)}{\beta_0}.$$
```

De forma que sólo hay que hacer una modificación  mínima al algoritmo de descenso en gradiente
para el caso no regularizado.

### Selección de coeficiente de regularización


Seleccionamos $\lambda$ para minimizar el error de predicción,
es decir, para mejorar nuestro modelo ajustado en cuanto a sus 
predicciones.

- No tiene sentido intentar escoger $\lambda>0$ usando el error
de entrenamiento. La razón es que siempre que aumentamos $\lambda$, obtenemos
un valor mayor de la suma de cuadrados / devianza del modelo, pues $\lambda$ más
grande implica que pesa menos la minimización de la suma de cuadrados /devianza
en el problema de la minimización. En otras palabras, los coeficientes tienen
una penalización más fuerte, de modo que el mínimo que se alcanza es mayor
en términos de devianza.
- Intentamos escoger $\lambda$ de forma que se minimice el error de predicción,
o el error de prueba (que estima el error de predicción).



#### Ejemplo (simulación) {-}

Regresamos a nuestro problema original simulado de clasificación. La función {\em glmnet}
se encarga de estandarizar variables y escoger un rango adecuado de
penalizaciones $\lambda$:



```{r}
library(glmnet)
mod.ridge <- glmnet(x=salida$mat.entrena, y=salida$g.entrena, 
  alpha = 0,
  family='binomial', intercept = F, nlambda=50)
```

En primer lugar, observamos cómo se encogen los coeficientes para
distintos valores de $\lambda$:
```{r}
dim(coef(mod.ridge))
plot(mod.ridge, xvar='lambda')
```

Para escoger el valor adecuado de $\lambda$, calculamos la devianza 
bajo la muestra de prueba:


```{r}
devianza  <- function(prob, y){
  -2*mean(y*log(prob)+(1-y)*log(1-prob)    )
}

dat.r <- plyr::ldply(1:50, function(i){
  dat.prueba.r <- data.frame(i=i, lambda=mod.ridge$lambda[i],
    prob.hat.1=h(salida$mat.prueba%*%as.numeric(coef(mod.ridge)[,i][-1])), 
  clase = salida$g.prueba)
  dat.prueba.r
})

devianza.prueba <- plyr::ddply(dat.r, c('i','lambda'), summarise, 
  dev = devianza(prob.hat.1, clase))
qplot(log(devianza.prueba$lambda), devianza.prueba$dev)
```

Buscamos entonces minimizar la devianza (evaluada en la muestra de prueba),
que corresponde a tomar un valor de $\lambda$ alrededor de exp(-2).
El modelo final queda como sigue:

```{r}
mod.ridge$lambda
pred.prueba.final <- salida$mat.prueba%*%(coef(mod.ridge)[ , 40][-1])
#table(pred.prueba.final > 0.5, salida$g.prueba)
#prop.table(table(pred.prueba.final > 0.5, salida$g.prueba), margin=2)
#prop.table(table(pred.prueba.final > 0.1, salida$g.prueba), margin=2)
```









## Entrenamiento, Validación y Prueba

El enfoque que vimos arriba, en donde dividemos la muestra en dos
partes al azar, es la manera más fácil de seleccionar modelos. En general,
el proceso es el siguiente:

- Una parte con los que ajustamos todos
los modelos que nos interesa. Esta es la **muestra de entrenamiento**
- Una parte como muestra de prueba, con el que evaluamos el desempeño
de cada modelo ajustado en la parte anterior. En este contexto, 
a esta muestra se le llama **muestra de validación}**.
- Posiblemente una muestra adicional independiente, que 
llamamos **muestra de prueba**, con la que hacemos una evaluación
final del modelo seleccionado arriba. Es una buena idea 
apartar esta muestra si el proceso de validación incluye muchos métodos
con varios parámetros afinados (como la $\lambda$ de regresión ridge).

\begin{figure}
\includegraphics[width=10cm]{./imagenes/div_muestra.pdf}
\end{figure}

Cuando tenemos datos abundantes, este enfoque es el usual. Por ejemplo,
podemos dividir la muestra en 50-25-25 por ciento. Ajustamos modelos
con el primer 50\%, evaluamos y seleccionamos con el segundo 25\% y finalmente,
si es necesario, evaluamos el modelo final seleccionado con la muestra 
final de 25\%. 

La razón de este proceso es que así podemos ir y venir entre
entrenamiento y validación, buscando mejores enfoques y modelos, y
no ponemos en riesgo la estimación final del error. (Pregunta: ¿por qué
probar agresivamente buscando mejorar el error de validación podría
ponder en riesgo la estimación final del error del modelo seleccionado? )

### Validación cruzada

En muchos casos, no queremos apartar una muestra de prueba para seleccionar modelos,
pues no tenemos muchos datos, por ejemplo, al dividir la muestra obtendríamos
un modelo relativamente {\em malo} en relación al que resulta de todos los datos.
Un criterio para seleccionar la regularización adecuada
es el de {\bf validación cruzada}, que es un método computacional
para producir una estimación interna (usando sólo muestra de entrenamiento)
del error de predicción.


En validación cruzada (con $k$ vueltas), 
construimos al azar una partición, con tamaños similares, de la muestra de entrenamiento
$ {\mathcal L}=\{ (x_i,y_i)\}_{i=1}^n$:

$$ {\mathcal L}={\mathcal L}_1\cup {\mathcal L}_2\cup\cdots\cup {\mathcal L}_k.$$

\begin{figure}
\includegraphics[width=8cm]{./imagenes/div_muestra_cv.pdf}
\end{figure}


Construimos $k$ modelos distintos, digamos $\hat{f}_j$, usando solamente
la muestra ${\mathcal L}-{\mathcal L}_j$. Este modelo lo evaluamos
usando la parte que no usamos, ${\mathcal L}_j$, para obtener una 
estimación honesta del error del modelo $\hat{f}_k$, a la que denotamos
por $\hat{e}_j$. 

Notemos entonces que tenemos $k$ estimaciones del error
$\hat{e}_1,\ldots, \hat{e}_k$, una para cada uno de los modelos que construimos.
La idea ahora es que

- Cada uno de los modelos $\hat{f}_j$ es similar al modelo ajustado
con toda la muestra $\hat{f}$, de forma que podemos pensar
que cada una de las estimaciones $\hat{e}_j$ es un estimador del error de $\hat{f}$.
- Dado el punto anterior, podemos construir una mejor estimación
promediando las $k$ estimaciones anteriores, para obtener:
$$\widehat{cv} = \frac{1}{k} \sum_{j=1}^k \hat{e}_j.$$



Por ejemplo, el paquete *glmnet* incluye la función
*cv.glmnet*, que hace los $k$ ajustes para cada una
de las lambdas:

```{r}
library(glmnet)
cv.mod.ridge <- cv.glmnet(x=salida$mat.entrena, y=salida$g.entrena, 
  alpha = 0,
  family='binomial', intercept = F, nfolds = 10, nlambda=50)
plot(cv.mod.ridge)
cv.mod.ridge$lambda.min
cv.mod.ridge$lambda.1se
```

Nótese que la estimación del error de predicción por validación
cruzada incluye un error de estimación (intervalos). Esto nos
da dos opciones para escoger la lambda final:

- Escoger la que de el mínimo valor de error por validación cruzada
- Escoger la lambda más grande *que no esté a más de 1 error estándar
del mínimo.*

En la gráfica anterior se muestran las dos posibilidades. La razón del segundo
criterio es tomar el modelo más simple que tenga error consistente con el
mejor modelo.



¿Cómo se desempeña validación cruzada como estimación del error?
```{r}
#comp.dat <- data.frame(log.lambda=log(devianza.prueba$lambda),
#  devianza.prueba = devianza.prueba$dev,
#  devianza.vc = cv.mod.ridge$cvm)
#comp.dat.m <- melt(comp.dat, id.vars='log.lambda')
#ggplot(comp.dat.m, aes(x=log.lambda, y=value, colour=variable)) +
#  geom_point()
```


Vemos que la estimación en algunos casos no es tan buena, aún cuando
todos los datos fueron usados. Pero el mínimo se encuentra en lugares
muy similares. 

La razón es que validación cruzada en realidad considera 
perturbaciones del conjunto de entrenamiento, de forma que lo que 
intenta evaluar el error producido, para cada lambda, sobre 
distintas muestras de entrenamiento.

En realidad nosotros queremos evaluar el error de predicción del
modelo que ajustamos. Validación cruzada es más un estimador
del error esperado de predicción sobre los modelos que ajustaríamos
con distintas muestras de entrenamiento.

El resultado es que:

- Usamos validación cruzada para escoger la complejidad adecuada
de la familia de modelos que consideramos.
- Como estimación del error de predicción del modelo que ajustamos,
validación cruzada es más seguro que usar el error de entrenamiento, que
muchas veces puede estar fuertemente sesgado hacia arriba. Sin embargo, lo
mejor en este caso es utilizar una muestra de prueba.


### Ejercicio {-}
En regresión ridge, coeficientes de variables correlacionadas se encogen
juntos. Esta es una buena estrategia, pues precisamente cuando
tenemos variables correlacionadas, los coeficientes estimados pueden
sufrir de varianza alta (si tenemos dos variables similares,
¿cuánta influencia le asignamos a cada una por las predicciones?).

Construye un ejemplo simulado ($p=2$)con dos bloques de variables que tengan
variación común, y la respuesta es una variable $y$ que es función lineal
de las cuatro variables dadas. ¿Cómo se comportan los coeficientes?










